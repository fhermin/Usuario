{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d27e6-452e-4b36-8d04-f530d61a6e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Descomenta la siguiente línea para instalar las dependencias\n",
    "# %pip install matplotlib numpy ipywidgets ipympl\n",
    "# Reiniciar jupyter lab después de instalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a937d-7774-4e34-8665-0092f0b6f961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077fdfb-cef5-46c9-b0f6-ad5b3d8f5e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f978916-15cf-403b-bf31-380c48d983a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rc(\"text\", usetex=False)\n",
    "mpl.rc(\"font\", size=12)\n",
    "mpl.rc(\"figure\", dpi=100, figsize=(5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25cd45-90e1-423c-ba74-5ad2f9cb8fe2",
   "metadata": {},
   "source": [
    "En esta parte del curso discutiremos sobre modelos basados en reflejos, en particular utilizando técnicas de **Aprendizaje máquina**.\n",
    "\n",
    "- El aprendizaje máquina (*machine learning* en inglés) es el proceso de transformar *datos* en un *modelo*.\n",
    "- Las técnicas que discutiremos también se pueden aplicar a distintas clases de modelos.\n",
    "- Los modelos basados en reflejos se caracterizan por un proceso de inferencia muy rápido.\n",
    "\n",
    "El siguiente esquema muestra la forma general de estos modelos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./modelo.png\" />\n",
    "</center>\n",
    "\n",
    "Nuestro modelo es un predictor $f$ que toma una entrada $x$ y produce alguna salida $y$.\n",
    "\n",
    "La entrada puede usualmente ser arbitraria (una imágen, una oración, etc.), pero la forma de la salida $y$ está restringida por lo general. Estas restricciones son las que determinan el tipo de predicción que se lleva a cabo.\n",
    "\n",
    "Por el momento, discutiremos sobre dos tipos de predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4af6f7-6c1e-46b4-8abc-0c0a6607f1f9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Clasificación binaria</b>\n",
    "</center>\n",
    "\n",
    "En este tipo de problemas de predicción, la salida $y$ se restringe a dos valores, uno positivo ($+1$) y uno negativo ($-1$).\n",
    "\n",
    "En estos contextos, el predictor $f$ es llamado *clasificador* y la salida $y$ es llamada *etiqueta*, *clase* o *categoría*.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./clasificacion-binaria.png\" />\n",
    "</center>\n",
    "\n",
    "Un problema clásico modelado como un problema de clasificación binaria es el de determinar si un correo electrónico es \"spam\" o no (*¿spam o ham?*). La entrada en este caso es una representación del contenido del correo y la salida es $+1$ (spam) o $-1$ (ham).\n",
    "\n",
    "Otra aplicación es detección de fraudes, de entrada es la información de la transacción bancaria y la salida si es o no fraudulenta.\n",
    "\n",
    "Otros problemas donde nos interese separar las entradas en dos grupos son compatibles con clasificación binaria.\n",
    "\n",
    "Una generalización de la clasificación binaria es la clasificación multiclase, donde la salida $y$ puede ser una de $K$ posibilidades, por ejemplo, en la clasificación de dígitos escritos a mano, entra una imágen al clasificador y sale un valor $y \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d921b8-a658-443f-9db1-12721cf1f593",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Regresión</b>\n",
    "</center>\n",
    "\n",
    "En este tipo de problemas de predicción, la salida $y$ se restringe a el conjunto de los reales $\\mathbb{R}$.\n",
    "\n",
    "En estos contextos, la salida $y$ es usualmente llamada *respuesta* u *objetivo*.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./regresion.png\" />\n",
    "</center>\n",
    "\n",
    "La distinción principal entre la clasificación y la regresión es que la primera tiene salidas *discretas*, mientras que la segunda tiene salidas *continuas*.\n",
    "\n",
    "Los problemas de regresión han sido históricamente utilizados para interpolar o extrapolar información. Desde el movimiento de los cuerpos celestes en el cosmos 😌, hasta el uso de herramientas matemáticas para justificar la eugenesia 🤢.\n",
    "\n",
    "Podemos por ejemplo aplicar esta técnica a predicción de índices de desarrollo a partir de una imágen satelital de tu vecindario, predecir el precio de una casa a partir de una variedad de factores, o incluso predecir el tiempo en que llegará la pizza a tu casa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb34994-03af-4a99-a0ad-5ac3d86e08fd",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Predicción estructurada</b>\n",
    "</center>\n",
    "\n",
    "En este tipo de problemas de predicción, vamos a (mas o menos) capturar el resto de los tipos. La salida $y$ puede ser un objeto complejo, como una oración o una imagen, por lo que el espacio de posibles salidas es enorme.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./estructurada.png\" />\n",
    "</center>\n",
    "\n",
    "Una aplicación es la traducción automática, a partir de una oración en un lenguaje, predecir su traducción en otra.\n",
    "\n",
    "También podría ser el caso de traducir una imágen a una oración que la describe.\n",
    "\n",
    "En una imagen, identificar los rostros humanos que aparecen en ella.\n",
    "\n",
    "Quizá a algunos de nosotros nos puedan parecer mágicas/distópicas/futuristas estas aplicaciones, sin embargo en muchos casos, un problema de predicción estructurada puede replantearse como una secuencia de problemas de clasificación multiclase. Por ejemplo, para predecir una oración, podemos predecir una palabra a la vez, de izquierda a derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4d694-8a41-451b-b100-2335c208c679",
   "metadata": {},
   "source": [
    "# Regresión lineal\n",
    "\n",
    "Supongamos que recolectamos o se nos provee con **datos de entrenamiento**, denotados $\\mathcal{D}_\\mathrm{train}$, que consisten de un conjunto de ejemplos. Cada **ejemplo** (también llamado punto, instancia o caso) consiste de una entrada $x$ y una salida $y$.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./regresion-framework.png\" />\n",
    "</center>\n",
    "\n",
    "Un algoritmo de aprendizaje toma los datos de entrenamiento y produce un modelo $f$ (nuestro predictor). Estos modelos nos van a permitir hacer predicciones sobre nuevas entradas nunca antes vistas durante el entrenamiento. En el diagrama de arriba, se alimenta la entrada $3$ al predictor para obtener $f(3)$ (en este ejemplo $2.71$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553da14-7e75-43ef-854e-4dcbff3aa525",
   "metadata": {},
   "source": [
    "En Python podemos definir los datos de entrenamiento como una lista de parejas $(x, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98aca37-15b0-437b-9c86-402d5d085037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (4, 3),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ab46e-8086-4f28-93f9-0edc2fac1b21",
   "metadata": {},
   "source": [
    "Supongamos que nuestro predictor es muy simple y corresponde a una función lineal. En este ejemplo vamos a definir en Python explícitamente el modelo, sin embargo mas adelante vamos a **aprenderlo** automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc72308-00f4-450d-a0bd-c1535ca2dfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.57 * x + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935fabb-7e5d-4c3a-9ff8-6c1cf7c45087",
   "metadata": {},
   "source": [
    "Para visualizar los datos de entrenamiento, el modelo y la predicción de una nueva entrada $3$ podemos graficar $y$ contra $x$ en el plano cartesiano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acf892-4584-4a3e-8a6a-f92b9ac702a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs = [x for x, y in train_examples]\n",
    "ys = [y for x, y in train_examples]\n",
    "\n",
    "xfs = np.linspace(0, 5)\n",
    "yfs = f(xfs)\n",
    "\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.xaxis.set_ticks(range(6))\n",
    "ax.yaxis.set_ticks(range(5))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_title(\"Predicción con regresión lineal\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs, ys, c=\"C0\", zorder=1, label=\"Entrenamiento\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2, label=\"Modelo\")\n",
    "ax.scatter(3, f(3), c=\"C1\", zorder=3, label=\"Predicción\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649231e-fe0c-4fca-9072-50a02739026b",
   "metadata": {},
   "source": [
    "Hay tres decisiones de diseño que debemos hacer para especificar el algoritmo de aprendizaje por completo:\n",
    "1. ¿Qué predictores $f$ se le permite al algoritmo producir? ¿Solo lineas o también curvas? En otras palabras, ¿Cuál es la **clase de hipótesis**?\n",
    "2. ¿Cómo juzga el algoritmo de aprendizaje cuál predictor es bueno? En otras palabras, ¿Cuál es la **función de pérdida**?\n",
    "3. ¿Cómo el algoritmo de aprendizaje realmente encuentra el mejor predictor? En otras palabras, ¿Cuál es el **algoritmo de optimización**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e38bf-fc08-4c72-b872-22761d24737b",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Clase de hipótesis</b>\n",
    "</center>\n",
    "\n",
    "Consideremos la primer decisión de diseño, sobre cuál es la clase de hipótesis.\n",
    "\n",
    "Un posible predictor es el que se muestra en rojo, donde cruza el eje $y$ en $1$ y la pendiente es $0.57$. Otro predictor es el que se muestra en morado, donde cruza el eje $y$ en $2$ y la pendiente es $0.2$.\n",
    "\n",
    "En general, consideremos todos los predictores de la forma\n",
    "\n",
    "$$f(x) = w_1 + w_2x$$\n",
    "\n",
    "que cruzan el eje $y$ en $w_1$ y la pendiente es $w_2$, donde ambos valores pueden ser números reales arbitrarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ef78b-dad7-4072-8aa3-1cd45ccfd4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs = np.linspace(0, 5)\n",
    "f1ys = 1.0 + 0.57 * xs\n",
    "f2ys = 2.0 + 0.20 * xs\n",
    "\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.xaxis.set_ticks(range(6))\n",
    "ax.yaxis.set_ticks(range(5))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_title(\"Dos predictores lineales distintos\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.plot(xs, f1ys, c=\"red\")\n",
    "ax.plot(xs, f2ys, c=\"purple\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03025130-7fe4-4e73-8957-99514a422000",
   "metadata": {},
   "source": [
    "Ahora generalicemos esta clase de hipótesis utilizando notación vectorial. Empacamos la altura y pendiente en un vector al que llamaremos **vector de pesos**, estos corresponderán a los parámetros del modelo.\n",
    "\n",
    "$$\\mathbf{w} = [w_1, w_2]$$\n",
    "\n",
    "De forma similar, definimos una función $\\phi$ llamada **extractor de características**, el cuál toma $x$ y lo convierte en un **vector de características**.\n",
    "\n",
    "$$\\phi(x) = [1, x]$$\n",
    "\n",
    "Ahora podemos definir el predictor $f_{\\mathbf{w}}$ que toma $x$ y calcula el producto punto entre el vector de pesos $\\mathbf{w}$ y el vector de características $\\phi(x)$.\n",
    "\n",
    "$$f_{\\mathbf{w}}(x) = \\mathbf{w}\\cdot\\phi(x)$$\n",
    "\n",
    "Finalmente, definimos la clase de hipótesis $\\mathcal{F}$ como el conjnuto de todos los posibles predictores $f_{\\mathbf{w}}$.\n",
    "\n",
    "$$\\mathcal{F} = \\left\\{ f_{\\mathbf{w}} : \\mathbf{w} \\in \\mathbb{R}^2 \\right\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d8cef-9380-461b-943a-e8b063902f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array([1, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc314e-ac1a-43e5-9d84-d714ef97e65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phi(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3deb8c-baf1-44ed-9bcf-9e92d4217bf6",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Función de pérdida</b>\n",
    "</center>\n",
    "\n",
    "La siguiente decisión de diseño es cómo juzgar cada uno de los predictores.\n",
    "\n",
    "Intuitivamente, un predictor es bueno si puede ajustarse a los datos de entrenamiento. Consideremos la separación entre la salida predecida $f_{\\mathbf{w}}(x)$ y la salida real $y$, a esto lo conocemos como el **residual**.\n",
    "\n",
    "Consideramos entonces la función de pérdida sobre un ejemplo dado con respecto a $\\mathbf{w}$ como el cuadrado del residual, de tal manera que los residuales grandes representan una pérdida mayor que los residuales pequeños.\n",
    "\n",
    "$$\\mathrm{Loss}(x, y, \\mathbf{w}) = (f_{\\mathbf{w}}(x)-y)^2$$\n",
    "\n",
    "A esta función de pérdida le llamamos pérdida cuadrada y mide que tan malo es el modelo $f$ para un ejemplo particular.\n",
    "\n",
    "Para cada ejemplo, tenemos una pérdida calculada a partir del vector de pesos (que representan nuestro modelo) y el ejemplo. Ahora definimos la pérdida de los datos de entrenamiento (o error de entrenamiento) que toma el promedio de las pérdidas de cada ejemplo en los datos de entrenamiento.\n",
    "\n",
    "$$\\mathrm{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_{\\mathrm{train}}|}\\sum_{(x, y)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(x, y, \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73fcdb-11b5-4125-9de8-2fb52504a7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return (w.dot(phi(x)) - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322d1bd-8ec7-41e3-834f-2d0afc847bed",
   "metadata": {},
   "source": [
    "Veamos cuáles son las pérdidas de los ejemplos del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebae37-bd31-4b7a-a788-1a0e9c296511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[0], np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe8e79-a908-4e5d-b449-602cb01203e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[1], np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe350e2-a850-48d2-bfc3-b8fde5a9204b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[2], np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b61852-cbf5-4300-948a-4fec0918fed5",
   "metadata": {},
   "source": [
    "Y ahora calculemos la pérdida sobre todos los ejemplos del conjnuto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e391304-9ee5-4366-a503-4a814d9fc6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum(loss(x, y, w) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5bb82-67ec-4f40-9956-d576d0eab931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss(np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ceac8f-6c20-4b32-a2e3-bde34ad9db06",
   "metadata": {},
   "source": [
    "*¿Qué pasa con la pérdida cuadrática promedio cuando tenemos valores atípicos (outliers) en los datos e entrenamiento?*\n",
    "\n",
    "Podemos visualizar cómo se comporta la pérdida de entrenamiento graficando su valor para varios vectores de pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4a794-b0a8-4e7e-be02-4ece37f1ec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1min, w1max = (-10, 10)\n",
    "w2min, w2max = (-10, 10)\n",
    "samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3aaaca-ee75-4965-a07b-014514234535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss_vec(w1, w2):\n",
    "    total = sum(((w1 + w2 * x) - y) ** 2 for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a430f-f5bf-498a-8ed1-3cdd0410923c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "w1, w2 = np.meshgrid(\n",
    "    np.linspace(w1min, w1max, samples),\n",
    "    np.linspace(w2min, w2max, samples),\n",
    ")\n",
    "tl = train_loss_vec(w1, w2)\n",
    "\n",
    "ax.set_title(\"Pérdida de entrenamiento promedio\")\n",
    "ax.set_xlabel(\"$w_1$\")\n",
    "ax.set_ylabel(\"$w_2$\")\n",
    "ax.set_zlabel(\"TrainLoss\")\n",
    "\n",
    "ax.plot_surface(\n",
    "    w1,\n",
    "    w2,\n",
    "    tl,\n",
    "    rstride=1,\n",
    "    cstride=1,\n",
    "    antialiased=False,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"none\",\n",
    "    linewidth=0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e82d22-6dd5-4545-abf1-870de8cf5922",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Algoritmo de optimización</b>\n",
    "</center>\n",
    "\n",
    "Ahora discutimos la tercer decisión de diseño, cómo podemos calcular el mejor predictor.\n",
    "\n",
    "Para responder esta pregunta, nos podemos olvidar de que estamos trabajando con regresión lineal y aprendizaje máquina. Simplemente tenemos una función objetivo $\\mathrm{TrainLoss}(\\mathbf{w})$ que queremos minimizar.\n",
    "\n",
    "Comenzamos con una estimación de $\\mathbf{w}$ y mejoramos la estimación de forma iterativa para hacer que su valor de acuerdo a la función objetivo disminuya. Finalmente nos detenemos cuando la estimación sea suficientemente buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61d2e6-bc73-4de0-b9ad-4d81cf4502c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_weights():\n",
    "    return np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779a80f-fbb1-4a6b-a2b2-c7cac0095e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38aaa4-bd7a-46e9-be4b-de6655f0c4a3",
   "metadata": {},
   "source": [
    "Para lograr la mejora progresiva de la estimación consideramos el gradiente de la función objetivo, el cuál corresponde a un vector que apunta hacia dónde disminuye más empinadamente la función objetivo.\n",
    "\n",
    "$$\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2c501-fa83-4684-87bf-18e2d50f0c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_train_loss(w):\n",
    "    total = sum(2 * (w.dot(phi(x)) - y) * phi(x) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf78d1-284c-4dae-819d-c99401032b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient_train_loss(np.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6383208-f7a8-4715-b833-f4fe7fa9cd4b",
   "metadata": {},
   "source": [
    "El proceso de optimización iterativa es llamado **descenso de gradiente**, funciona de la siguiente manera:\n",
    "\n",
    "1. Inicializamos $\\mathbf{w}$ a algún valor (p.ej. vector de ceros)\n",
    "2. Luego realizamos la siguiente actualización $T$ veces, llamada la cantidad de épocas (epochs en inglés):\n",
    "3. Tomamos el vector $\\mathbf{w}$ y le restamos un valor escalado del gradiente, donde el factor de escalamiento (o tamaño de paso) se denota $\\eta$, este parámetro nos permite controlar qué tan agresivo es el descenso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b8164-3366-4cd6-8457-2bcbef6ca171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(F, gradientF, init):\n",
    "    w = init()\n",
    "    eta = 0.1\n",
    "    for t in range(10):\n",
    "        value = F(w)\n",
    "        gradient = gradientF(w)\n",
    "        w = w - eta * gradient\n",
    "        print(f\"epoch {t}: w = {w}, F(w) = {value:.4f}, gradientF = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f2737-95fd-4265-a3dd-2719252e7f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient_descent(train_loss, gradient_train_loss, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9587b-dfd3-4a9d-a805-4ad4725f0313",
   "metadata": {},
   "source": [
    "¿Cómo se minimiza la pérdida de entrenamiento promedio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa385dc2-9972-417a-8a91-8ff13fdd0b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent_viz(F, gradientF, init):\n",
    "    w = init()\n",
    "    w1s = [w[0]]\n",
    "    w2s = [w[1]]\n",
    "    tls = []\n",
    "    eta = 0.1\n",
    "    for t in range(100):\n",
    "        value = F(w)\n",
    "        tls.append(value)\n",
    "        gradient = gradientF(w)\n",
    "        w = w - eta * gradient\n",
    "        w1s.append(w[0])\n",
    "        w2s.append(w[1])\n",
    "    tls.append(F(np.array([w1s[-1], w2s[-1]])))\n",
    "    return np.array(w1s), np.array(w2s), np.array(tls)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "w1, w2 = np.meshgrid(\n",
    "    np.linspace(-1, 2, samples),\n",
    "    np.linspace(-1, 2, samples),\n",
    ")\n",
    "tl = train_loss_vec(w1, w2)\n",
    "\n",
    "ax.set_title(\"Minimización de TrainLoss\")\n",
    "ax.set_xlabel(\"$w_1$\")\n",
    "ax.set_ylabel(\"$w_2$\")\n",
    "ax.set_zlabel(\"TrainLoss\")\n",
    "\n",
    "w1s, w2s, tls = gradient_descent_viz(train_loss, gradient_train_loss, initial_weights)\n",
    "\n",
    "ax.plot(w1s, w2s, tls, color=\"white\", alpha=0.5, zorder=10)\n",
    "\n",
    "ax.plot_surface(\n",
    "    w1,\n",
    "    w2,\n",
    "    tl,\n",
    "    rstride=1,\n",
    "    cstride=1,\n",
    "    antialiased=False,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"none\",\n",
    "    linewidth=0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f14260-6128-48e6-bb91-2281fb7bbdb8",
   "metadata": {},
   "source": [
    "## Repaso\n",
    "\n",
    "**¿Qué predictores son posibles?**\n",
    "\n",
    "Construimos una *clase de hipótesis*.\n",
    "\n",
    "En lo que discutimos, consideramos *funciones lineales*,\n",
    "$$\\mathcal{F} = \\{ f_\\mathbf{w}(x) = \\mathbf{w}\\cdot\\phi(x) \\}$$\n",
    "$$\\phi(x) = [1, x]$$\n",
    "\n",
    "**¿Qué tan bueno es un predictor?**\n",
    "\n",
    "Construimos una *función de pérdida*.\n",
    "\n",
    "En lo que discutimos, consideramos la *pérdida cuadrática*,\n",
    "$$\\mathrm{Loss}(x, y, \\mathbf{w}) = (f_\\mathbf{w}(x) - y)^2$$\n",
    "\n",
    "**¿Cómo calcular el mejor predictor?**\n",
    "\n",
    "Construimos un *algoritmo de optimización*.\n",
    "\n",
    "En lo que discutimos, consideramos el *descenso de gradiente*,\n",
    "$$\\mathbf{w} \\gets \\mathbf{w} - \\eta\\nabla\\mathrm{TrainLoss}(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f05093-5078-497b-8d74-144ec112c70e",
   "metadata": {},
   "source": [
    "# Clasificación lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c60741-e5b2-4145-90eb-142adb216d6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Al igual que en la regresión lineal, recolectamos o se nos provee con **datos de entrenamiento**, ($\\mathcal{D}_\\mathrm{train}$), pero ahora estos datos consisten de **ejemplos** con dos entradas reales $(x_1, x_2)$ y una salida binaria $y$ ($+1$ o $-1$).\n",
    "\n",
    "<center>\n",
    "    <img src=\"./classification-framework.png\" />\n",
    "</center>\n",
    "\n",
    "También vamos a querer construir un algoritmo de aprendizaje que tome los datos de entrenamiento y produzca un modelo $f$, al que llamamos **clasificador**. Estos modelos nos van a permitir clasificar nuevas entradas nunca antes vistas durante el entrenamiento. En el diagrama de arriba, se alimenta la entrada $[2, 0]$ al clasificador para obtener $f([2, 0])$ (en este ejemplo $-1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a5015-4054-4262-aa1a-b999018377e6",
   "metadata": {},
   "source": [
    "En Python podemos definir los datos de entrenamiento como una lista de parejas $(x, y)$, donde $x$ se representa como una tupla de dos elementos $(x_1, x_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3c496-e57a-4ca4-87d2-84ba73887c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    ((0, 2), 1),\n",
    "    ((-2, 0), 1),\n",
    "    ((1, -1), -1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17bbe5-aa70-4e05-9f47-a35f9e8794b7",
   "metadata": {},
   "source": [
    "Supongamos que nuestro clasificador es muy simple y corresponde a una relación lineal entre la primera y segunda componente de las entradas.\n",
    "\n",
    "Por el momento, usamos la recta identidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb99409-6911-4c10-af1f-70f3e2cf6e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x1):\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15dfac2-2aa1-41df-a717-968655eaef91",
   "metadata": {},
   "source": [
    "Para visualizar los datos de entrenamiento, el modelo y la clasificación de una nueva entrada $[2,0]$ podemos graficar $x_1$ contra $x_2$ en el plano cartesiano. El modelo entonces parte el espacio de entrada en dos, uno donde se clasifican los puntos con $+1$ y otro donde se clasifican con $-1$.\n",
    "\n",
    "A la frontera entre estas dos divisiones se le llama **frontera de decisión**. Graficamos una flecha perpendicular a la frontera para indicar qué región es la positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124a26e-5e5c-45f9-acf5-44e1eaa719cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs1plus = [x1 for (x1, x2), y in train_examples if y == +1]\n",
    "xs2plus = [x2 for (x1, x2), y in train_examples if y == +1]\n",
    "\n",
    "xs1minus = [x1 for (x1, x2), y in train_examples if y == -1]\n",
    "xs2minus = [x2 for (x1, x2), y in train_examples if y == -1]\n",
    "\n",
    "xfs = np.linspace(-3, 3)\n",
    "yfs = f(xfs)\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.xaxis.set_ticks(range(-3, 4))\n",
    "ax.yaxis.set_ticks(range(-3, 4))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs1plus, xs2plus, c=\"C0\", zorder=1, label=\"$+1$\")\n",
    "ax.scatter(xs1minus, xs2minus, c=\"C2\", zorder=1, label=\"$-1$\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2)\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0, 0),\n",
    "    xytext=(-0.6, +0.6),\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<-\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    ")\n",
    "ax.scatter(2, 0, c=\"white\", edgecolors=\"C2\", zorder=3, linewidth=2)\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a1010-8127-481f-ad1a-1a91cdad4946",
   "metadata": {},
   "source": [
    "Retomamos las mismas tres decisiones de diseño para nuestro algoritmo de aprendizaje.\n",
    "\n",
    "1. ¿Qué clasificadores $f$ se le permite al algoritmo producir? ¿La frontera de decisión debe ser recta o puede curvarse? En otras palabras, ¿Cuál es la **clase de hipótesis**?\n",
    "2. ¿Cómo juzga el algoritmo de aprendizaje cuál clasificador es bueno? En otras palabras, ¿Cuál es la **función de pérdida**?\n",
    "3. ¿Cómo el algoritmo de aprendizaje realmente encuentra el mejor clasificador? En otras palabras, ¿Cuál es el **algoritmo de optimización**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fbd5d-6ae6-431b-9d0a-0705ad46bf54",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Clase de hipótesis</b>\n",
    "</center>\n",
    "\n",
    "\n",
    "Vamos a considerar todos los clasificadores de la forma\n",
    "\n",
    "$$f(x_1, x_2) = \\mathrm{sign}(w_1x_1 + w_2x_2)$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\\mathrm{sign}(z) = \\begin{cases}\n",
    "+1 &\\text{si } z > 0 \\\\\n",
    "-1 &\\text{si } z < 0 \\\\\n",
    "0  &\\text{si } z = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Donde la frontera de decisión consiste de todos los puntos $(x_1, x_2)$ tal que $w_1x_1+w_2x_2 = 0$.\n",
    "\n",
    "Intentemos entender mejor lo que esta clase de hipótesis representa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05341dd9-a496-42c9-876d-382ea85b0dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550b48f-d827-406c-aa57-f12987819569",
   "metadata": {},
   "source": [
    "Al igual que en el caso de la regresión lineal, planteamos la clase de hipótesis utilizando notación vectorial.\n",
    "\n",
    "La clasificación lineal se parametriza por un vector de pesos $\\mathbf{w}$.\n",
    "\n",
    "$$\\mathbf{w} = [w_1, w_2]$$\n",
    "\n",
    "Definimos un extractor de características $\\phi$ como la identidad.\n",
    "\n",
    "$$\\phi(x) = [x_1, x_2]$$\n",
    "\n",
    "Y procedemos a definir el clasificador $f_\\mathbf{w}$ que toma $x$ de entrada y calcula el signo del producto punto entre el vector de pesos $\\mathbf{w}$ y el vector de características $\\phi(x)$.\n",
    "\n",
    "$$f_\\mathbf{w}(x) = \\mathrm{sign}(\\mathbf{w}\\cdot\\phi(x))$$\n",
    "\n",
    "Podemos encontrar un sentido geométrico para estos clasificadores.\n",
    "\n",
    "- Considerando $\\mathbf{w}$ y $\\phi(x)$ como vectores, su producto punto es un factor positivo multiplicado por el coseno del ángulo $\\theta$ entre ellos. Si $\\theta < \\pi/2$, entonces $\\cos(\\theta)$ es positivo y el signo en $f_\\mathbf{w}$ ignora las magnitudes. Cuando los vectores son perpendiculares, es porque $\\phi(x)$ se encuentra justo sobre la frontera de decisión. \n",
    "- Al graficar los ejemplos como puntos en tres dimensiones (la etiqueta siendo la tercera dimensión) el clasificador $f_\\mathbf{w}$ corresponde a un plano en el espacio que siempre corta al origen. Todos los puntos por arriba del plano se clasifican positivos y todos por debajo se clasifican negativos.\n",
    "\n",
    "Finalmente, definimos la clase de hipótesis $\\mathcal{F}$ como el conjunto de todos los posibles predictores $f_{\\mathbf{w}}$.\n",
    "\n",
    "$$\\mathcal{F} = \\left\\{ f_{\\mathbf{w}} : \\mathbf{w} \\in \\mathbb{R}^2 \\right\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f276c-4341-449f-9657-3c242b31a743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7c8ab-8285-4f1e-872d-2fbaa7f4decd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sign(z):\n",
    "    if z > 0:\n",
    "        return +1\n",
    "    if z < 0:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ec06d-d311-4d2b-b4c6-2936d066cba7",
   "metadata": {},
   "source": [
    "Consideremos el clasificador con $\\mathbf{w} = [0.5, 1]$, ¿Qué tal clasifica los datos de entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf6ffa-b1ff-45a0-a6f0-00051f507ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = np.array([0.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b954b5-b182-4f06-82ae-6234222eb6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs1plus = [x1 for (x1, x2), y in train_examples if y == +1]\n",
    "xs2plus = [x2 for (x1, x2), y in train_examples if y == +1]\n",
    "\n",
    "xs1minus = [x1 for (x1, x2), y in train_examples if y == -1]\n",
    "xs2minus = [x2 for (x1, x2), y in train_examples if y == -1]\n",
    "\n",
    "xfs = np.linspace(-3, 3)\n",
    "yfs = -(w[0] / w[1]) * xfs\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.xaxis.set_ticks(range(-3, 4))\n",
    "ax.yaxis.set_ticks(range(-3, 4))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_title(\"Predicción con clasificador lineal\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs1plus, xs2plus, c=\"C0\", zorder=1, label=\"$+1$\")\n",
    "ax.scatter(xs1minus, xs2minus, c=\"C2\", zorder=1, label=\"$-1$\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2)\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0, 0),\n",
    "    xytext=(w[0], w[1]),\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<-\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea944ee0-ac9a-4af9-bfd4-9988874e621c",
   "metadata": {},
   "source": [
    "Vemos que dos ejemplos los clasifica correctamente, pero un tercero con etiqueta positiva es clasificado como negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f866333-8a1e-4759-9436-e1d40f2262d1",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Función de pérdida</b>\n",
    "</center>\n",
    "\n",
    "Utilizamos una función de pérdida llamada **cero-uno** que regresa cero cuando la etiqueta del ejemplo y el resultado de la clasificación coinciden, y que regresa uno cuando difieren.\n",
    "\n",
    "$$\\mathrm{Loss}_{0-1}(x, y, \\mathbf{w}) = \\mathbf{1}[f_\\mathbf{w}(x) \\not= y]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd275b-c3ba-40ac-8a44-2dfbcaf093db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return int(sign(phi(x).dot(w)) != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5a6ad-5823-4bef-b173-5106292ee408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[0], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b788e-cb48-419b-8ae6-120f138fb854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[1], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8eb0e2-5dc0-4bfc-a2e1-942ccec0c632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[2], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8230c-00a4-4b1b-b1ce-24793748c252",
   "metadata": {},
   "source": [
    "$$f_\\mathbf{w}(x) = \\mathrm{sign}(\\mathbf{w}\\cdot \\phi(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9594f-8ff2-4772-aece-580dcc76936a",
   "metadata": {},
   "source": [
    "La pérdida total es simplemente el promedio de la pérdida cero-uno sobre todos los ejemplos de entrenamiento, en este ejemplo sería $\\frac{1}{3}$.\n",
    "\n",
    "Antes de discutir la tercer decisión de diseño (el algoritmo de optimización), veamos otras funciones de pérdida que pueden resultar convenientes de utilizar.\n",
    "\n",
    "Recordemos que cuando $\\mathbf{w}\\cdot\\phi(x)$ es positiva, la etiqueta predecida es $+1$ debido a que el ángulo interno entre los dos vectores es menor a $90^\\circ$.\n",
    "\n",
    "La magnitud absoluta de este escalar es proporcional a la distancia entre el ejemplo de entrenamiento y la frontera de decisión. Por lo tanto, podemos considerar este escalar como una métrica de qué tan seguros estamos en predecir $+1$. Si la distancia entre un ejemplo y la frontera de decisión es pequeña, entonces pequeños cambios en el modelo pueden producir cambios en la predicción. Si la distancia es muy grande, entonces el modelo pudiera cambiar sin alterar la predicción.\n",
    "\n",
    "Podemos adecuar este concepto utilizando el **margen**, el cuál representa qué tan correcta es la predicción. Basta multiplicar $\\mathbf{w}\\cdot\\phi(x)$ por la etiqueta de los datos de entrenamiento asociada a $x$. Con esta modificación, un margen negativo indica errores de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407400ce-1818-4865-a57e-76c4a19c4250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x, y in train_examples:\n",
    "    x = np.array(x)\n",
    "    print(f\"phi(x)={x}, margen={w.dot(phi(x))*y}\")\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs1plus = [x1 for (x1, x2), y in train_examples if y == +1]\n",
    "xs2plus = [x2 for (x1, x2), y in train_examples if y == +1]\n",
    "\n",
    "xs1minus = [x1 for (x1, x2), y in train_examples if y == -1]\n",
    "xs2minus = [x2 for (x1, x2), y in train_examples if y == -1]\n",
    "\n",
    "xfs = np.linspace(-3, 3)\n",
    "yfs = -(w[0] / w[1]) * xfs\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.xaxis.set_ticks(range(-3, 4))\n",
    "ax.yaxis.set_ticks(range(-3, 4))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_title(\"Predicción con clasificador lineal\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs1plus, xs2plus, c=\"C0\", zorder=1, label=\"$+1$\")\n",
    "ax.scatter(xs1minus, xs2minus, c=\"C2\", zorder=1, label=\"$-1$\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2)\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0, 0),\n",
    "    xytext=(w[0], w[1]),\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<-\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fc5bc-aa99-4275-bb80-cb0047d6307a",
   "metadata": {},
   "source": [
    "Podemos reescribir la pérdida cero-uno en términos del margen, recordemos que el margen es positivo cuando la clasificación es correcta.\n",
    "\n",
    "$$\\mathrm{Loss}_\\text{0-1}(x, y, \\mathbf{w}) = \\mathbf{1}[f_\\mathbf{w}(x) \\not= y]$$\n",
    "\n",
    "$$\\mathrm{Loss}_\\text{0-1}(x, y, \\mathbf{w}) = \\mathbf{1}[\\underbrace{(\\mathbf{w}\\cdot\\phi(x))y}_\\text{margen} \\leq 0]$$\n",
    "\n",
    "Grafiquemos el margen contra la pérdida cero uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d80800-25eb-4c22-ab21-4c337f82194c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ms = np.linspace(-3, 3, 200)\n",
    "ls = 1.0 * (ms <= 0.0)\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.set_xlabel(\"margen $(\\mathbf{w}\\cdot\\phi(x))y$\")\n",
    "ax.set_ylabel(\"$\\mathrm{Loss}(x, y, \\mathbf{w})$\")\n",
    "\n",
    "ax.plot(ms, ls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526a6ad-4428-4884-bb5e-116d29cb5327",
   "metadata": {},
   "source": [
    "Una desventaja clara con utilizar la pérdida cero-uno es que tiene gradientes cero, esto significa que utilizar un algoritmo de optimización como el descenso de gradiente, fácilmente caemos en un mínimo local.\n",
    "\n",
    "Conceptualmente, quisieramos que cuando el margen sea negativo, la pérdida sea mayor a cero, mientras que si el margen es positivo, la pérdida sea cercana a cero. Podemos tomar pérdidas más grandes entre mas negativo sea el margen y tender a cero a partir de que el margen sea positivo.\n",
    "\n",
    "Discutamos dos funciones de pérdida alternativas que capturan el espíritu de lo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877652f-2f59-4aeb-8889-54358c8a3ad8",
   "metadata": {},
   "source": [
    "La pérdida de articulación (*Hinge loss*) penaliza linealmente los márgenes no-positivos y considera una pérdida de cero para los márgenes positivos.\n",
    "\n",
    "Una forma usual de la pérdida de articulación es:\n",
    "\n",
    "$$\\mathrm{Loss}_\\mathrm{hinge}(x, y, \\mathbf{w}) = \\max\\{1 - (\\mathbf{w}\\cdot\\phi(x))y, 0\\}$$\n",
    "\n",
    "El $1$ en la relación lineal nos permite tener un poco de holgura, queremos que el clasificador no solo sea correcto, si no que clasifique con un margen positivo. Un algoritmo de aprendizaje intentaría encontrar un clasificador que produzca margenes más grandes a pesar de haber encontrado un clasificador correcto.\n",
    "\n",
    "Visualicemos cómo se comporta la pérdida conforme cambia el margen de acuerdo a las dos pérdidas que hemos visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0757a82-b1d1-4c85-90ae-e2120c323f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ms = np.linspace(-3, 3, 200)\n",
    "ls1 = 1.0 * (ms <= 0.0)\n",
    "ls2 = ((1.0 - ms) > 0) * (1.0 - ms)\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.set_xlabel(\"margen $(\\mathbf{w}\\cdot\\phi(x))y$\")\n",
    "ax.set_ylabel(\"$\\mathrm{Loss}(x, y, \\mathbf{w})$\")\n",
    "\n",
    "ax.plot(ms, ls1, label=\"$\\mathrm{Loss}_\\mathrm{0-1}$\")\n",
    "ax.plot(ms, ls2, label=\"$\\mathrm{Loss}_\\mathrm{hinge}$\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63419e-cac1-4da4-802a-0653c6fb9fd4",
   "metadata": {},
   "source": [
    "Observemos que la pérdida de articulación acota por arriba la pérdida cero-uno, también son exactamente iguales para márgenes mayores a uno.\n",
    "\n",
    "Cuando reducimos el valor de la pérdida de articulación, también reducimos en general la pérdida cero-uno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b482f53-f11f-45b0-8d07-b34dc4ea493c",
   "metadata": {},
   "source": [
    "La pérdida logística (*Logistic loss*) es otra alternativa, la idea es intentar aumentar el margen aunque ya sea mayor a uno.\n",
    "\n",
    "$$\\mathrm{Loss}_\\mathrm{logistic}(x, y, \\mathbf{w}) = \\ln{\\left(1 + e^{-(\\mathbf{w}\\cdot\\phi(x))y}\\right)}$$\n",
    "\n",
    "Visualicemos las tres pérdidas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f3d70-80fe-4b22-acb7-1aae4fd4cd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ms = np.linspace(-3, 3, 200)\n",
    "ls1 = 1.0 * (ms <= 0.0)\n",
    "ls2 = ((1.0 - ms) > 0) * (1.0 - ms)\n",
    "ls3 = np.log(1 + np.exp(-ms))\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.set_xlabel(\"margen $(\\mathbf{w}\\cdot\\phi(x))y$\")\n",
    "ax.set_ylabel(\"$\\mathrm{Loss}(x, y, \\mathbf{w})$\")\n",
    "\n",
    "ax.plot(ms, ls1, label=\"$\\mathrm{Loss}_\\mathrm{0-1}$\")\n",
    "ax.plot(ms, ls2, label=\"$\\mathrm{Loss}_\\mathrm{hinge}$\")\n",
    "ax.plot(ms, ls3, label=\"$\\mathrm{Loss}_\\mathrm{logistic}$\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135384b-b76a-4292-a094-bc160b93fd48",
   "metadata": {},
   "source": [
    "Elegimos la pérdida de articulación para lo que resta de la clase.\n",
    "\n",
    "¿Cómo podemos implementarla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b479e6-91bf-49fa-bf30-dd18fe44f65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return max(1 - phi(x).dot(w) * y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ac1ee-4e25-40e7-9bab-04b0e72b4b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20eba1-801f-465c-8700-fe2333b06428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b6327-be40-4739-9d21-8b9ce283b37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[0], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3f157-f468-4c89-b3d5-88e868522834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[1], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2d571-098b-40d6-97dc-c45c9bd0c288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[2], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a92f3d-040c-4cda-ab69-84ad5161388a",
   "metadata": {},
   "source": [
    "Calculemos el gradiente de la pérdida de articulación, consideramos dos casos, cuando el márgen es menor a uno y cuando no.\n",
    "\n",
    "$$\\mathrm{Loss}_\\mathrm{hinge}(x, y, \\mathbf{w}) = \\max\\{1 - (\\mathbf{w}\\cdot\\phi(x))y, 0\\}$$\n",
    "\n",
    "$$\\nabla\\mathrm{Loss}_\\mathrm{hinge}(x, y, \\mathbf{w}) = \\begin{cases}\n",
    "-\\phi(x)y &\\text{si}\\ 1-(\\mathbf{w}\\cdot\\phi(x))y > 0\\\\\n",
    "0 &\\text{en otro caso}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8583ea-4422-4239-9e19-9e6221846075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum(max(1 - w.dot(phi(x)) * y, 0) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902222dd-fd32-44d3-bf25-2e802701bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78e80a-d4d5-42ba-94da-a5e4830b10d4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Algoritmo de optimización</b>\n",
    "</center>\n",
    "\n",
    "¡Utilicemos el descenso de gradiente!\n",
    "\n",
    "Una vez que definimos la pérdida de entrenamiento utilizando la pérdida de articulación, solo tenemos que implementar el gradiente de esta nueva pérdida de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a4e62-013d-442f-81b3-e6d0756f2ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_train_loss(w):\n",
    "    total = sum(\n",
    "        -phi(x) * y if 1 - w.dot(phi(x)) * y > 0 else 0 for x, y in train_examples\n",
    "    )\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a854290-a652-4312-9529-f72f2f60c101",
   "metadata": {},
   "source": [
    "La implementación de los pesos iniciales y el algoritmo de descenso de gradiente pueden quedar intactos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd6582-77c4-4e57-bb39-e5c0a864ec48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient_descent(train_loss, gradient_train_loss, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eef779-3d02-46a9-9704-4245b52a6739",
   "metadata": {},
   "source": [
    "## Repaso\n",
    "\n",
    "**¿Qué predictores son posibles?**\n",
    "\n",
    "Construimos una *clase de hipótesis*.\n",
    "\n",
    "En lo que discutimos, consideramos *fontera de decisión lineal*.\n",
    "\n",
    "**¿Qué tan bueno es un predictor?**\n",
    "\n",
    "Construimos una *función de pérdida*.\n",
    "\n",
    "Consideramos la pérdida cero-uno, de articulación y logística.\n",
    "\n",
    "**¿Cómo calcular el mejor predictor?**\n",
    "\n",
    "Construimos un *algoritmo de optimización*.\n",
    "\n",
    "Usamos *descenso de gradiente*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bd9b6-c1a8-447e-a171-9defef651913",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Descenso de gradiente estocástico\n",
    "\n",
    "En los dos tipos de problemas que discutimos en clases pasadas, utilizamos el descenso de gradiente, un algoritmo de propósito general para optimizar la pérdida de entrenamiento.\n",
    "\n",
    "$$\\mathrm{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(x, y, \\mathbf{w})$$\n",
    "\n",
    "*Descenso de gradiente:*\n",
    "1. Inicializamos $\\mathbf{w}$ a algún valor\n",
    "2. Luego realizamos la siguiente actualización $T$ veces:\n",
    "3. $\\mathbf{w} \\gets \\mathbf{w} - \\eta\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w})$\n",
    "\n",
    "\n",
    "Sin embargo, este algoritmo es muy lento, la pérdida de entrenamiento realiza una sumatoria sobre todos los datos de entrenamiento. Si tenemos millones de ejemplos, entonces cada cálculo del gradiente requiere considerar cada uno de estos millones de ejemplos. Todo esto antes de ajustar el modelo aunque sea poquito.\n",
    "\n",
    "¿Podemos ir mejorando el modelo antes de procesar todos los ejemplos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53ba9a-59ca-4a78-8dc6-1fe531f1a9d7",
   "metadata": {},
   "source": [
    "La respuesta es el descenso de gradiente estocástico (*SGD* por *Stochastic Gradient Descent*)\n",
    "\n",
    "En lugar de iterar sobre todos los ejemplos para calcular un gradiente y dar un paso, el SGD itera sobre los ejemplos y actualiza los pesos en cada ejemplo.\n",
    "\n",
    "Cada actualización no es tan buena ya que la actualización del modelo es en base a un solo ejemplo, sin embargo, podemos hacer más actualizaciones de esta forma.\n",
    "\n",
    "*Descenso de gradiente estocástico:*\n",
    "1. Inicializamos $\\mathbf{w}$ a algún valor\n",
    "2. Luego realizamos lo siguiente $T$ veces\n",
    "3. Para cada ejemplo $(x, y)$ en $\\mathcal{D}_\\mathrm{train}$:\n",
    "4. $\\mathbf{w}\\gets\\mathbf{w}-\\eta\\nabla_\\mathbf{w}\\mathrm{Loss}(x, y, \\mathbf{w})$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./sgd.png\" />\n",
    "</center>\n",
    "\n",
    "¿Prefieren *calidad* o *cantidad*? El SGD prefiere cantidad\n",
    "\n",
    "Solo hay un detalle... Elegir el tamaño de paso puede ser complicado.\n",
    "\n",
    "Entre más grande sea $\\eta$ más agresivo es el descenso, podemos llegar al mínimo más rápido, pero también enfierrarnos de más y nunca llegar a él.\n",
    "\n",
    "Si elegimos un $\\eta$ muy pequeño, las actualizaciones serían mas estables, pero llegaríamos al mínimo demasiado lento. En el caso extremo cuando $\\eta = 0$ el modelo nunca es actualizado.\n",
    "\n",
    "Una forma de mitigar esto es variar $\\eta$ en cada actualización del modelo. En particular, vamos a considerar que iniciamos con $\\eta=1$ y luego ajustarla en función a la cantidad de actualizaciones realizadas hasta el momento $u$,\n",
    "$$\\eta \\gets \\frac{1}{\\sqrt{u}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02da2c-a2a7-4f7a-83a7-059575700bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "xs = np.linspace(1, 10, 100)\n",
    "ys = 1.0 / np.sqrt(xs)\n",
    "\n",
    "ax.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d576312-754f-40a4-b9c3-2ec0a86cefe7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450636a-5a61-4139-97cf-746624ca065e",
   "metadata": {},
   "source": [
    "Consideremos el problema de regresión lineal, pero utilizando datos de entrenamiento aleatorios.\n",
    "\n",
    "Vamos a considerar un modelo lineal para generar los datos, representado por un vector de cinco componentes (pensemos en un hiperplano en cinco dimensiones).\n",
    "\n",
    "Este será nuestro vector de pesos \"real\", pero ni la pérdida ni el SGD sabrán que valores tiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3e1b9-b78b-4344-81e4-424843ab0f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trueW = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8f5f5-5c3b-4b9f-8419-c9ee754f092d",
   "metadata": {},
   "source": [
    "Generamos ejemplos donde la entrada $x$ es un vector aleatorio donde cada componente sigue la distribución normal. Luego obtenemos la salida que predice el vector de persos real y le agregamos ruido, también con distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733940f4-6c4e-40f0-8ebe-325415e3a595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = np.random.randn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d73f3b-884d-4dd9-89fd-da68435928cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ecf9b-aaf5-4467-86cb-9a4242e18bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.hist(v, bins=100, density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88828f48-2849-4966-a76d-97a938dc05d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    x = np.random.randn(len(trueW))\n",
    "    y = trueW.dot(x) + np.random.normal(0, 2)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b570-b876-473a-b02d-9172b9216f2c",
   "metadata": {},
   "source": [
    "Generemos los datos de entrenamiento con un millón de ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63045236-4f04-4b84-a282-69bc6fbec52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6ea4c-cc25-4855-b681-21f8f735d732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [generate() for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3ab29-240f-481f-9c33-17ed141444d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa76096-8acd-4bff-8afa-e87154afde5d",
   "metadata": {},
   "source": [
    "Recordemos cómo modelamos en programas de Python la regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed6bc4-09c7-4ff9-85ec-6abe10608434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4ec1f-4237-4f28-85ab-5ccf889bb313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_weights():\n",
    "    return np.zeros(len(trueW))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7e5ff-bcde-4983-84c9-f8a11ac4d327",
   "metadata": {},
   "source": [
    "Para descenso de gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a769a8-82f1-464d-8dfa-cfcdb635c174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum((w.dot(phi(x)) - y) ** 2 for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e613c-89ea-47d7-bfae-a1a0e10516dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_train_loss(w):\n",
    "    total = sum(2 * (w.dot(phi(x)) - y) * phi(x) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7d611-b501-491a-bee9-635e0d31107d",
   "metadata": {},
   "source": [
    "Para descenso de gradiente estocástico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad6e70-7942-42fc-93ee-34733ebe6239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(w, i):\n",
    "    x, y = train_examples[i]\n",
    "    return (w.dot(phi(x)) - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf887d-ac3a-43e7-b41b-373e01ec6f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_loss(w, i):\n",
    "    x, y = train_examples[i]\n",
    "    return 2 * (w.dot(phi(x)) - y) * phi(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34571d-4c99-4523-8c89-1d8f1aa76cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GDepochs = 10\n",
    "SGDepochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd10b3f-5220-4724-9010-86d23e6a6da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradientDescent(F, gradientF, init):\n",
    "    w = init()\n",
    "    eta = 0.1\n",
    "    for t in range(GDepochs):\n",
    "        value = F(w)\n",
    "        gradient = gradientF(w)\n",
    "        w = w - eta * gradient\n",
    "        print(f\"epoch {t}: w = {w}, F(w) = {value:.4f}, gradientF = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6c21a-6a13-4061-add1-2dd9cd8d0820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stochasticGradientDescent(f, gradientf, n, init):\n",
    "    w = init()\n",
    "    numUpdates = 0\n",
    "    for t in range(SGDepochs):\n",
    "        for i in range(n):\n",
    "            value = f(w, i)\n",
    "            gradient = gradientf(w, i)\n",
    "            numUpdates += 1\n",
    "            eta = 1.0 / np.sqrt(numUpdates)\n",
    "            w = w - eta * gradient\n",
    "        print(f\"epoch {t}: w = {w}, F(w) = {value:.4f}, gradientF = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b2d83-d571-4b64-95ea-8c4115185731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "gradientDescent(train_loss, gradient_train_loss, initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb04b41-a186-4ad2-995a-74dcafd7ea0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "stochasticGradientDescent(loss, gradient_loss, len(train_examples), initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cdd89-a41d-4240-83dd-13ee2859eef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss(np.array([0.98478211, 2.02829034, 2.98268202, 4.05039687, 5.0458254]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede8753-4265-4fb6-a010-cf1a90e44557",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Optimización de grupo distribucionalmente robusta\n",
    "\n",
    "También llamada *group DRO* por *group distributionally robust optimization*.\n",
    "\n",
    "---\n",
    "\n",
    "Hasta ahora nos hemos enfocado en encontrar modelos que minimizan la pérdida de entrenamiento, la cuál es el promedio de la pérdida sobre los ejemplos de entrenamiento.\n",
    "\n",
    "Considerar el promedio puede parecer intuitivo y quizá “lo correcto”... Veamos por qué esto puede nos er el caso...\n",
    "\n",
    "Ver video: http://gendershades.org/\n",
    "\n",
    "El proyecto *Gender Shades* es solo uno de los muchos ejemplos que apuntan a un problema general y sistémico: los modelos de aprendizaje máquina, típicamente optimizados para maximizar la precisión promedio, pueden resultar en precisiones pobres para ciertos **grupos** (subpoblaciones).\n",
    "\n",
    "Estos grupos pueden ser definidos por clases protegidas. De acuerdo a la CNDH algunos temas relativos a grupos vulnerables en México son:\n",
    "- Personas Migrantes\n",
    "- Personas en Situación de Víctimas\n",
    "- Niñas, Niños y Adolescentes\n",
    "- Jóvenes, Personas Mayores y Familias\n",
    "- Personas desaparecidas y no localizadas\n",
    "- Sexualidad, Salud y VIH\n",
    "- Igualdad entre Mujeres y Hombres\n",
    "- Periodistas y Personas Defensoras Civiles\n",
    "- Contra la Trata de Personas\n",
    "- Pueblos y Comunidades Indígenas y Afrodescendientes\n",
    "- Personas con Discapacidad\n",
    "- Prevención de la Tortura\n",
    "- Personas Privadas de la Libertad\n",
    "- Derechos Económicos, Sociales, Culturales y Ambientales\n",
    "\n",
    "(Ver http://informe.cndh.org.mx/menu.aspx?id=80002)\n",
    "\n",
    "Puedes consultar información sobre el trabajo legislativo de nuestros representantes que constituyen la comisión de *Atención a Grupos Vulnerables* en la *Cámara de Diputados*: https://web.diputados.gob.mx/inicio/comision?Oid=f8cfe362-3718-4e81-b0f6-9a85f0a12ecc\n",
    "\n",
    "En un caso particular, un hombre negro con el nombre de Robert Julian-Borchak Williams fue arrestado por error debido a una coincidencia incorrecta con otro hombre negro capturado en un video de vigilancia. Este error fue cometido por un sistema de reconocimiento facial.\n",
    "\n",
    "(Ver https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html)\n",
    "\n",
    "Con el proyecto Gender Shades, podemos ver que precisiones más bajas para algunos grupos pueden tener consecuencias en la vida real, como más arrestos erróneos, lo cuál agravia aún más las desigualdades que existen actualmente en nuestra sociedad.\n",
    "\n",
    "Vamos a enfocarnos en las disparidades de rendimiento en el aprendizaje máquina y cómo podemos mitigarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10331c5-c2ab-44c1-969f-92013aa25be4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Regresión lineal con grupos</b>\n",
    "</center>\n",
    "\n",
    "Consideremos la regresión lineal.\n",
    "\n",
    "Comenzamos con datos de entrenamiento, pero ahora cada ejemplo consiste no solo de la entrada $x$ y la salida $y$, si no también de un **grupo** $g$.\n",
    "\n",
    "En este ejemplo, suponemos que tenemos dos grupos $A$ y $B$.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./dro00.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95add0-e89a-473a-92b1-4705d7095646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    (1, 4, \"A\"),\n",
    "    (2, 8, \"A\"),\n",
    "    (5, 5, \"B\"),\n",
    "    (6, 6, \"B\"),\n",
    "    (7, 7, \"B\"),\n",
    "    (8, 8, \"B\"),\n",
    "]\n",
    "\n",
    "pred_x = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63cc3a-c56f-45f6-bb56-45812e8a70d4",
   "metadata": {},
   "source": [
    "$$\\phi(x) = [x]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dd668-1827-423d-8c5f-baac434ddbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array([x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa982f6a-13cf-4781-9b7f-fc60d67d2aea",
   "metadata": {},
   "source": [
    "$$f_\\mathbf{w}(x) = \\mathbf{w}\\cdot\\phi(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c4486-85eb-4882-ade3-75b5a6582695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    return np.dot(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000ee2-e3e7-40a1-a0a7-939a3de489eb",
   "metadata": {},
   "source": [
    "$$\\mathrm{Loss}(\\mathbf{w}, x, y) = (f_\\mathbf{w}(x)-y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc0a13-13d3-4503-bba2-573a42d85792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return (predict(w, x) - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b881cc-73a3-4bf2-aeca-aef6117811ad",
   "metadata": {},
   "source": [
    "$$\\mathrm{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y,g)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(\\mathbf{w}, x, y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71e26-a378-49be-9aa9-808b29016169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum(loss(w, x, y) for x, y, g in train_examples)\n",
    "    return total / len(train_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc2cb7-8e1c-4fb5-b299-761550e954ec",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\mathrm{Loss}(\\mathbf{w}, x, y) &= \\nabla_\\mathbf{w} (f_\\mathbf{w}(x)-y)^2 \\\\\n",
    "                                                 &= \\nabla_\\mathbf{w} ((\\mathbf{w}\\cdot\\phi(x))-y)^2 \\\\\n",
    "                                                 &= 2\\left((\\mathbf{w}\\cdot\\phi(x))-y\\right)\\nabla_\\mathbf{w}\\left((\\mathbf{w}\\cdot\\phi(x))-y\\right)  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\nabla_\\mathbf{w}\\left((\\mathbf{w}\\cdot\\phi(x))-y\\right)  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\left( \\nabla_\\mathbf{w}(\\mathbf{w}\\cdot\\phi(x)) - \\nabla_\\mathbf{w}y\\right)  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\nabla_\\mathbf{w}(\\mathbf{w}\\cdot\\phi(x))  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\phi(x)  \\\\\n",
    "                                                 \\\\\n",
    "\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w}) &= \\nabla_\\mathbf{w}\\left(\\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y,g)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(\\mathbf{w}, x, y)\\right) \\\\\n",
    "                                                &= \\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y,g)\\in\\mathcal{D}_\\mathrm{train}} \\nabla_\\mathbf{w}\\mathrm{Loss}(\\mathbf{w}, x, y)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ecd59-a9e6-49e2-bb72-caa7f165c5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_loss(w, x, y):\n",
    "    return 2 * (predict(w, x) - y) * phi(x)\n",
    "\n",
    "\n",
    "def gradient_train_loss(w):\n",
    "    total = sum(gradient_loss(w, x, y) for x, y, g in train_examples)\n",
    "    return total / len(train_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9139b0-289e-4777-a4bc-ca4ae01ab326",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\eta &\\gets 0.1 \\\\\n",
    "\\mathbf{w} &\\gets [0] \\\\\n",
    "\\mathbf{w} &\\gets \\mathbf{w} - \\eta\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904165c-2ea1-4780-9e37-7c8585ddc38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradientDescent(loss, gradient_loss, eta=0.1, w=np.array([0.0]), N=20):\n",
    "    for i in range(N):\n",
    "        w = w - eta * gradient_loss(w)\n",
    "        # print(f\"epoch={i} w={w} loss={loss(w)} ∇loss={gradient_loss(w)}\")\n",
    "    return w, loss(w), gradient_loss(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f50dbe-2064-489d-aa07-4183cb1b6259",
   "metadata": {},
   "source": [
    "Obtenemos un buen modelo para los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65696754-543e-4a37-8c7c-40c048027414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w, lossW, gradientLossW = gradientDescent(train_loss, gradient_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b2af7-67be-4ae9-ade9-bfb5aa66eac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "xs1 = np.array([x for x, y, g in train_examples if g == \"A\"])\n",
    "ys1 = np.array([y for x, y, g in train_examples if g == \"A\"])\n",
    "xs2 = np.array([x for x, y, g in train_examples if g == \"B\"])\n",
    "ys2 = np.array([y for x, y, g in train_examples if g == \"B\"])\n",
    "\n",
    "pxs = np.linspace(0, 8, 100)\n",
    "pys = w[0] * pxs\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_xlim(0, 8.25)\n",
    "ax.set_ylim(0, 8.25)\n",
    "ax.set_xticks(range(9))\n",
    "ax.set_yticks(range(9))\n",
    "\n",
    "ax.scatter(xs1, ys1, label=\"A\")\n",
    "ax.scatter(xs2, ys2, label=\"B\")\n",
    "ax.plot(pxs, pys, color=\"black\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425d95e-b50d-4aff-8d9f-4d5ccb6d6cea",
   "metadata": {},
   "source": [
    "¿Cómo se ve la minimización de la pérdida promedio de entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38f563-511d-461f-ada4-da3759b3f174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "ws = np.linspace(0, 2, 100)\n",
    "ls = np.array([train_loss(np.array(w)) for w in ws])\n",
    "\n",
    "ax.set_xlabel(\"$w$\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(0, 80.25)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(0, 81, 20))\n",
    "\n",
    "ax.plot(ws, ls, c=\"r\", label=\"TrainLoss\")\n",
    "ax.scatter(w, lossW, c=\"r\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db3d33-c310-4018-b7e3-25e9218af1ce",
   "metadata": {},
   "source": [
    "Analicemos la pérdida promedio de entrenamiento **por grupo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e1a36-b0b2-485c-ad87-54cf7de8853e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss_g(w, group):\n",
    "    examples = [loss(w, x, y)\n",
    "                for x, y, g \n",
    "                in train_examples\n",
    "                if g == group]\n",
    "    return sum(examples) / len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6637f4c-f3f7-4965-a5ed-33ca7c7a7799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ws = np.linspace(0, 2, 100)\n",
    "lsA = np.array([train_loss_g(np.array(w), 'A') for w in ws])\n",
    "lsB = np.array([train_loss_g(np.array(w), 'B') for w in ws])\n",
    "\n",
    "ax.set_xlabel(\"$w$\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(0, 80.25)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(0, 81, 20))\n",
    "\n",
    "ax.plot(ws, lsA, label=\"$\\mathrm{TrainLoss}_A$\")\n",
    "ax.plot(ws, lsB, label=\"$\\mathrm{TrainLoss}_B$\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ede51-3e4b-49c0-8c1a-c2fb117d0591",
   "metadata": {},
   "source": [
    "\n",
    "$$\\mathrm{TrainLoss}_\\mathrm{max}(\\mathbf{w}) = \\max_g\\mathrm{TrainLoss}_g(\\mathbf{w})$$\n",
    "\n",
    "$$\\nabla_\\mathbf{w}\\mathrm{TrainLoss}_\\mathrm{max}(\\mathbf{w}) = \\nabla_\\mathbf{w}\\mathrm{TrainLoss}_{g^\\ast}(\\mathbf{w})$$\n",
    "\n",
    "$$g^\\ast = \\mathrm{argmax}_g\\mathrm{TrainLoss}_g(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f2e9a-64b7-468b-a738-451d4d676ae4",
   "metadata": {},
   "source": [
    "# Predictores no-lineales\n",
    "\n",
    "Hemos discutido las componentes principales de problemas de regresión y clasificación.\n",
    "\n",
    "1. Datos de entrenamiento $\\mathcal{D}$, nuestro conjunto de ejemplos con entradas $x$ y salidas $y$\n",
    "2. Clase de hipótesis $\\mathcal{F}$, el conjunto de todos los modelos de predicción que consideramos en el aprendizaje\n",
    "3. Función de pérdida $\\mathrm{Loss}(\\mathbf{w}, x, y)$, mide qué tan mal predice el modelo inducido por $\\mathbf{w}$ al ejemplo con entrada $x$ y salida $y$\n",
    "4. Algoritmo de optimización, como descenso de gradiente, nos permite encontrar un modelo que minimiza la pérdida de los datos de entrenamiento\n",
    "\n",
    "En particular, los modelos que discutimos son lineales, esto en el sentido que el cálculo fundamental que realizan es $\\mathbf{w}\\cdot\\phi(x)$.\n",
    "\n",
    "La función $\\phi$ es llamada el extractor de características y $\\phi(x)$ es llamado vector de características.\n",
    "\n",
    "En el caso de la regresión lineal, consideramos modelos parametrizados por un vector de pesos con dos componentes reales y $\\phi(x) = [1, x]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38e9b0-48c9-446b-bf99-b512f16fd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grafica(ws, colores):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    phi = lambda x: np.array([1.0, x])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = [np.array([np.dot(w, phi(x)) for x in xs]) for w in ws]\n",
    "    cs = mpl.colormaps[colores](np.linspace(0,1,len(ys)+2))[1:]\n",
    "    for y, c in zip(ys, cs):\n",
    "        ax.plot(xs, y, color=c)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ad3bf-e29d-467f-99dc-bbe76f7eaab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([2.0, 0.75 - 0.25 * i]) for i in range(5)], \"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8265-49fb-4b8c-819e-0400d4952ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([1 + i, 0.25]) for i in range(5)], \"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf25d1-8d04-438d-9378-2e25e9994f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([1.0 + 0.5 * i, 1.0 - 0.25 * i]) for i in range(5)], \"Oranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1e24b-0a2c-459d-8dc8-eaacb2e573df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([1.0 + 0.5 * np.random.rand() * i, 1.0 - 0.25 * np.random.rand() * i]) for i in range(5)], \"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952de5fe-6394-4279-8cfa-fe7381abcd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48729d29-95ba-488c-8e66-86f1e19bfeee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "fig = plt.figure();\n",
    "ax = plt.axes();\n",
    "def plotline(w1=2.0, w2=1.0):\n",
    "    ax.clear()\n",
    "    w = np.array([w1, w2])\n",
    "    phi = lambda x: np.array([1.0, x])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = np.array([np.dot(w, phi(x)) for x in xs])\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);\n",
    "    fig.show();\n",
    "\n",
    "w = interactive(plotline, w1=(0.0, 6.0), w2=(-10.0, 10.0))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af6d64-e43c-4509-9394-28927ec6b973",
   "metadata": {},
   "source": [
    "Consideremos utilizar un extractor de características cuadrático...\n",
    "\n",
    "$$\\phi(x) = [1, x, x^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f6a95-8a92-4681-9764-b172fc41431f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure();\n",
    "ax = plt.axes();\n",
    "def plotquadratic(w1=2.0, w2=1.0, w3=-0.5):\n",
    "    ax.clear()\n",
    "    w = np.array([w1, w2, w3])\n",
    "    phi = lambda x: np.array([1.0, x, x**2])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = np.array([np.dot(w, phi(x)) for x in xs])\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);\n",
    "    fig.show();\n",
    "\n",
    "w = interactive(plotquadratic, w1=(0.0, 6.0), w2=(-10.0, 10.0), w3=(-2.0,2.0))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce489bf-a85a-4286-a963-da8b6fcb3867",
   "metadata": {},
   "source": [
    "**¿Qué acaba de ocurrir?**\n",
    "\n",
    "El extractor de características $\\phi$ puede ser arbitrario. Entonces incluímos un término adicional $x^2$.\n",
    "\n",
    "Consideremos el cálculo principal del predictor $\\mathbf{w}\\cdot\\phi(x)$, esta relación es lineal con respecto a $\\mathbf{w}$ y $\\phi(x)$. Pero no necesita ser lineal en $x$, las entradas pudieran ser incluso objetos distintos a los vectores.\n",
    "\n",
    "El algoritmo de aprendizaje observa a $\\phi(x)$ pero no a $x$.\n",
    "\n",
    "**¿Qué otras maniacadas podemos conjurar?**\n",
    "\n",
    "$$\\phi(x) = [1, x, x^2, \\cos{(3x)}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82482fd-4dae-4870-a6cb-fec8ec6ba201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure();\n",
    "ax = plt.axes();\n",
    "def plotquadratic(w1=2.0, w2=1.0, w3=-0.5, w4=0.0):\n",
    "    ax.clear()\n",
    "    w = np.array([w1, w2, w3, w4])\n",
    "    phi = lambda x: np.array([1.0, x, x**2, np.cos(3*x)])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = np.array([np.dot(w, phi(x)) for x in xs])\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);\n",
    "    fig.show();\n",
    "\n",
    "w = interactive(\n",
    "    plotquadratic,\n",
    "    w1=(0.0, 6.0),\n",
    "    w2=(-10.0, 10.0),\n",
    "    w3=(-2.0, 2.0),\n",
    "    w4=(-1.0, 1.0),\n",
    ")\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a4c10-85f7-48a0-807a-dc937b3b7d87",
   "metadata": {},
   "source": [
    "Usualmente, la elección de características depende de la tarea de predicción que nos interesa resolver.\n",
    "\n",
    "Por ejemplo, si $x$ representa el tiempo y tenemos la sospecha que la salida real $y$ varía de acuerdo a alguna estructura periódica, entonces podemos incorporar características periódicas como el coseno para capturar esas tendencias.\n",
    "\n",
    "Cada característica puede representar algún tipo de estructura en los datos. Las características que incluyamos en nuestro predictor representan las propiedades que **pudieran** ser útiles para la predicción. Si una de estas no es útil, el algoritmo de aprendizaje puede *asignarle* un peso cercano a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfed41d-0629-4f2c-a8e9-993836f8f0cd",
   "metadata": {},
   "source": [
    "Retoma lo que discutimos sobre clasificadores lineales y considera las siguientes modificaciones:\n",
    "\n",
    "$$\\phi(x) = [x_1, x_2, x_1^2 + x_2^2]$$\n",
    "\n",
    "$$f_\\mathbf{w}(x) = \\mathrm{sign}(\\mathbf{w}\\cdot\\phi(x))$$\n",
    "\n",
    "Explora qué clasificamos con el modelo parametrizado por $\\mathbf{w} = [2, 2, -1]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
