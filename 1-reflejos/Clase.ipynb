{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d27e6-452e-4b36-8d04-f530d61a6e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Descomenta la siguiente l铆nea para instalar las dependencias\n",
    "# %pip install matplotlib numpy ipywidgets ipympl\n",
    "# Reiniciar jupyter lab despu茅s de instalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a937d-7774-4e34-8665-0092f0b6f961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077fdfb-cef5-46c9-b0f6-ad5b3d8f5e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f978916-15cf-403b-bf31-380c48d983a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.rc(\"text\", usetex=False)\n",
    "mpl.rc(\"font\", size=12)\n",
    "mpl.rc(\"figure\", dpi=100, figsize=(5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25cd45-90e1-423c-ba74-5ad2f9cb8fe2",
   "metadata": {},
   "source": [
    "En esta parte del curso discutiremos sobre modelos basados en reflejos, en particular utilizando t茅cnicas de **Aprendizaje m谩quina**.\n",
    "\n",
    "- El aprendizaje m谩quina (*machine learning* en ingl茅s) es el proceso de transformar *datos* en un *modelo*.\n",
    "- Las t茅cnicas que discutiremos tambi茅n se pueden aplicar a distintas clases de modelos.\n",
    "- Los modelos basados en reflejos se caracterizan por un proceso de inferencia muy r谩pido.\n",
    "\n",
    "El siguiente esquema muestra la forma general de estos modelos.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./modelo.png\" />\n",
    "</center>\n",
    "\n",
    "Nuestro modelo es un predictor $f$ que toma una entrada $x$ y produce alguna salida $y$.\n",
    "\n",
    "La entrada puede usualmente ser arbitraria (una im谩gen, una oraci贸n, etc.), pero la forma de la salida $y$ est谩 restringida por lo general. Estas restricciones son las que determinan el tipo de predicci贸n que se lleva a cabo.\n",
    "\n",
    "Por el momento, discutiremos sobre dos tipos de predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4af6f7-6c1e-46b4-8abc-0c0a6607f1f9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Clasificaci贸n binaria</b>\n",
    "</center>\n",
    "\n",
    "En este tipo de problemas de predicci贸n, la salida $y$ se restringe a dos valores, uno positivo ($+1$) y uno negativo ($-1$).\n",
    "\n",
    "En estos contextos, el predictor $f$ es llamado *clasificador* y la salida $y$ es llamada *etiqueta*, *clase* o *categor铆a*.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./clasificacion-binaria.png\" />\n",
    "</center>\n",
    "\n",
    "Un problema cl谩sico modelado como un problema de clasificaci贸n binaria es el de determinar si un correo electr贸nico es \"spam\" o no (*驴spam o ham?*). La entrada en este caso es una representaci贸n del contenido del correo y la salida es $+1$ (spam) o $-1$ (ham).\n",
    "\n",
    "Otra aplicaci贸n es detecci贸n de fraudes, de entrada es la informaci贸n de la transacci贸n bancaria y la salida si es o no fraudulenta.\n",
    "\n",
    "Otros problemas donde nos interese separar las entradas en dos grupos son compatibles con clasificaci贸n binaria.\n",
    "\n",
    "Una generalizaci贸n de la clasificaci贸n binaria es la clasificaci贸n multiclase, donde la salida $y$ puede ser una de $K$ posibilidades, por ejemplo, en la clasificaci贸n de d铆gitos escritos a mano, entra una im谩gen al clasificador y sale un valor $y \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d921b8-a658-443f-9db1-12721cf1f593",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Regresi贸n</b>\n",
    "</center>\n",
    "\n",
    "En este tipo de problemas de predicci贸n, la salida $y$ se restringe a el conjunto de los reales $\\mathbb{R}$.\n",
    "\n",
    "En estos contextos, la salida $y$ es usualmente llamada *respuesta* u *objetivo*.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./regresion.png\" />\n",
    "</center>\n",
    "\n",
    "La distinci贸n principal entre la clasificaci贸n y la regresi贸n es que la primera tiene salidas *discretas*, mientras que la segunda tiene salidas *continuas*.\n",
    "\n",
    "Los problemas de regresi贸n han sido hist贸ricamente utilizados para interpolar o extrapolar informaci贸n. Desde el movimiento de los cuerpos celestes en el cosmos , hasta el uso de herramientas matem谩ticas para justificar la eugenesia あ.\n",
    "\n",
    "Podemos por ejemplo aplicar esta t茅cnica a predicci贸n de 铆ndices de desarrollo a partir de una im谩gen satelital de tu vecindario, predecir el precio de una casa a partir de una variedad de factores, o incluso predecir el tiempo en que llegar谩 la pizza a tu casa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb34994-03af-4a99-a0ad-5ac3d86e08fd",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Predicci贸n estructurada</b>\n",
    "</center>\n",
    "\n",
    "En este tipo de problemas de predicci贸n, vamos a (mas o menos) capturar el resto de los tipos. La salida $y$ puede ser un objeto complejo, como una oraci贸n o una imagen, por lo que el espacio de posibles salidas es enorme.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./estructurada.png\" />\n",
    "</center>\n",
    "\n",
    "Una aplicaci贸n es la traducci贸n autom谩tica, a partir de una oraci贸n en un lenguaje, predecir su traducci贸n en otra.\n",
    "\n",
    "Tambi茅n podr铆a ser el caso de traducir una im谩gen a una oraci贸n que la describe.\n",
    "\n",
    "En una imagen, identificar los rostros humanos que aparecen en ella.\n",
    "\n",
    "Quiz谩 a algunos de nosotros nos puedan parecer m谩gicas/dist贸picas/futuristas estas aplicaciones, sin embargo en muchos casos, un problema de predicci贸n estructurada puede replantearse como una secuencia de problemas de clasificaci贸n multiclase. Por ejemplo, para predecir una oraci贸n, podemos predecir una palabra a la vez, de izquierda a derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4d694-8a41-451b-b100-2335c208c679",
   "metadata": {},
   "source": [
    "# Regresi贸n lineal\n",
    "\n",
    "Supongamos que recolectamos o se nos provee con **datos de entrenamiento**, denotados $\\mathcal{D}_\\mathrm{train}$, que consisten de un conjunto de ejemplos. Cada **ejemplo** (tambi茅n llamado punto, instancia o caso) consiste de una entrada $x$ y una salida $y$.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./regresion-framework.png\" />\n",
    "</center>\n",
    "\n",
    "Un algoritmo de aprendizaje toma los datos de entrenamiento y produce un modelo $f$ (nuestro predictor). Estos modelos nos van a permitir hacer predicciones sobre nuevas entradas nunca antes vistas durante el entrenamiento. En el diagrama de arriba, se alimenta la entrada $3$ al predictor para obtener $f(3)$ (en este ejemplo $2.71$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553da14-7e75-43ef-854e-4dcbff3aa525",
   "metadata": {},
   "source": [
    "En Python podemos definir los datos de entrenamiento como una lista de parejas $(x, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98aca37-15b0-437b-9c86-402d5d085037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    (1, 1),\n",
    "    (2, 3),\n",
    "    (4, 3),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ab46e-8086-4f28-93f9-0edc2fac1b21",
   "metadata": {},
   "source": [
    "Supongamos que nuestro predictor es muy simple y corresponde a una funci贸n lineal. En este ejemplo vamos a definir en Python expl铆citamente el modelo, sin embargo mas adelante vamos a **aprenderlo** autom谩ticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc72308-00f4-450d-a0bd-c1535ca2dfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.57 * x + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935fabb-7e5d-4c3a-9ff8-6c1cf7c45087",
   "metadata": {},
   "source": [
    "Para visualizar los datos de entrenamiento, el modelo y la predicci贸n de una nueva entrada $3$ podemos graficar $y$ contra $x$ en el plano cartesiano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acf892-4584-4a3e-8a6a-f92b9ac702a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs = [x for x, y in train_examples]\n",
    "ys = [y for x, y in train_examples]\n",
    "\n",
    "xfs = np.linspace(0, 5)\n",
    "yfs = f(xfs)\n",
    "\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.xaxis.set_ticks(range(6))\n",
    "ax.yaxis.set_ticks(range(5))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_title(\"Predicci贸n con regresi贸n lineal\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs, ys, c=\"C0\", zorder=1, label=\"Entrenamiento\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2, label=\"Modelo\")\n",
    "ax.scatter(3, f(3), c=\"C1\", zorder=3, label=\"Predicci贸n\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649231e-fe0c-4fca-9072-50a02739026b",
   "metadata": {},
   "source": [
    "Hay tres decisiones de dise帽o que debemos hacer para especificar el algoritmo de aprendizaje por completo:\n",
    "1. 驴Qu茅 predictores $f$ se le permite al algoritmo producir? 驴Solo lineas o tambi茅n curvas? En otras palabras, 驴Cu谩l es la **clase de hip贸tesis**?\n",
    "2. 驴C贸mo juzga el algoritmo de aprendizaje cu谩l predictor es bueno? En otras palabras, 驴Cu谩l es la **funci贸n de p茅rdida**?\n",
    "3. 驴C贸mo el algoritmo de aprendizaje realmente encuentra el mejor predictor? En otras palabras, 驴Cu谩l es el **algoritmo de optimizaci贸n**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e38bf-fc08-4c72-b872-22761d24737b",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Clase de hip贸tesis</b>\n",
    "</center>\n",
    "\n",
    "Consideremos la primer decisi贸n de dise帽o, sobre cu谩l es la clase de hip贸tesis.\n",
    "\n",
    "Un posible predictor es el que se muestra en rojo, donde cruza el eje $y$ en $1$ y la pendiente es $0.57$. Otro predictor es el que se muestra en morado, donde cruza el eje $y$ en $2$ y la pendiente es $0.2$.\n",
    "\n",
    "En general, consideremos todos los predictores de la forma\n",
    "\n",
    "$$f(x) = w_1 + w_2x$$\n",
    "\n",
    "que cruzan el eje $y$ en $w_1$ y la pendiente es $w_2$, donde ambos valores pueden ser n煤meros reales arbitrarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ef78b-dad7-4072-8aa3-1cd45ccfd4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs = np.linspace(0, 5)\n",
    "f1ys = 1.0 + 0.57 * xs\n",
    "f2ys = 2.0 + 0.20 * xs\n",
    "\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.xaxis.set_ticks(range(6))\n",
    "ax.yaxis.set_ticks(range(5))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_title(\"Dos predictores lineales distintos\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.plot(xs, f1ys, c=\"red\")\n",
    "ax.plot(xs, f2ys, c=\"purple\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03025130-7fe4-4e73-8957-99514a422000",
   "metadata": {},
   "source": [
    "Ahora generalicemos esta clase de hip贸tesis utilizando notaci贸n vectorial. Empacamos la altura y pendiente en un vector al que llamaremos **vector de pesos**, estos corresponder谩n a los par谩metros del modelo.\n",
    "\n",
    "$$\\mathbf{w} = [w_1, w_2]$$\n",
    "\n",
    "De forma similar, definimos una funci贸n $\\phi$ llamada **extractor de caracter铆sticas**, el cu谩l toma $x$ y lo convierte en un **vector de caracter铆sticas**.\n",
    "\n",
    "$$\\phi(x) = [1, x]$$\n",
    "\n",
    "Ahora podemos definir el predictor $f_{\\mathbf{w}}$ que toma $x$ y calcula el producto punto entre el vector de pesos $\\mathbf{w}$ y el vector de caracter铆sticas $\\phi(x)$.\n",
    "\n",
    "$$f_{\\mathbf{w}}(x) = \\mathbf{w}\\cdot\\phi(x)$$\n",
    "\n",
    "Finalmente, definimos la clase de hip贸tesis $\\mathcal{F}$ como el conjnuto de todos los posibles predictores $f_{\\mathbf{w}}$.\n",
    "\n",
    "$$\\mathcal{F} = \\left\\{ f_{\\mathbf{w}} : \\mathbf{w} \\in \\mathbb{R}^2 \\right\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d8cef-9380-461b-943a-e8b063902f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array([1, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc314e-ac1a-43e5-9d84-d714ef97e65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phi(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3deb8c-baf1-44ed-9bcf-9e92d4217bf6",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Funci贸n de p茅rdida</b>\n",
    "</center>\n",
    "\n",
    "La siguiente decisi贸n de dise帽o es c贸mo juzgar cada uno de los predictores.\n",
    "\n",
    "Intuitivamente, un predictor es bueno si puede ajustarse a los datos de entrenamiento. Consideremos la separaci贸n entre la salida predecida $f_{\\mathbf{w}}(x)$ y la salida real $y$, a esto lo conocemos como el **residual**.\n",
    "\n",
    "Consideramos entonces la funci贸n de p茅rdida sobre un ejemplo dado con respecto a $\\mathbf{w}$ como el cuadrado del residual, de tal manera que los residuales grandes representan una p茅rdida mayor que los residuales peque帽os.\n",
    "\n",
    "$$\\mathrm{Loss}(x, y, \\mathbf{w}) = (f_{\\mathbf{w}}(x)-y)^2$$\n",
    "\n",
    "A esta funci贸n de p茅rdida le llamamos p茅rdida cuadrada y mide que tan malo es el modelo $f$ para un ejemplo particular.\n",
    "\n",
    "Para cada ejemplo, tenemos una p茅rdida calculada a partir del vector de pesos (que representan nuestro modelo) y el ejemplo. Ahora definimos la p茅rdida de los datos de entrenamiento (o error de entrenamiento) que toma el promedio de las p茅rdidas de cada ejemplo en los datos de entrenamiento.\n",
    "\n",
    "$$\\mathrm{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_{\\mathrm{train}}|}\\sum_{(x, y)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(x, y, \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73fcdb-11b5-4125-9de8-2fb52504a7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return (w.dot(phi(x)) - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322d1bd-8ec7-41e3-834f-2d0afc847bed",
   "metadata": {},
   "source": [
    "Veamos cu谩les son las p茅rdidas de los ejemplos del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebae37-bd31-4b7a-a788-1a0e9c296511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[0], np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe8e79-a908-4e5d-b449-602cb01203e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[1], np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe350e2-a850-48d2-bfc3-b8fde5a9204b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[2], np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b61852-cbf5-4300-948a-4fec0918fed5",
   "metadata": {},
   "source": [
    "Y ahora calculemos la p茅rdida sobre todos los ejemplos del conjnuto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e391304-9ee5-4366-a503-4a814d9fc6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum(loss(x, y, w) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5bb82-67ec-4f40-9956-d576d0eab931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss(np.array([1.0, 0.57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ceac8f-6c20-4b32-a2e3-bde34ad9db06",
   "metadata": {},
   "source": [
    "*驴Qu茅 pasa con la p茅rdida cuadr谩tica promedio cuando tenemos valores at铆picos (outliers) en los datos e entrenamiento?*\n",
    "\n",
    "Podemos visualizar c贸mo se comporta la p茅rdida de entrenamiento graficando su valor para varios vectores de pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4a794-b0a8-4e7e-be02-4ece37f1ec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1min, w1max = (-10, 10)\n",
    "w2min, w2max = (-10, 10)\n",
    "samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3aaaca-ee75-4965-a07b-014514234535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss_vec(w1, w2):\n",
    "    total = sum(((w1 + w2 * x) - y) ** 2 for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a430f-f5bf-498a-8ed1-3cdd0410923c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "w1, w2 = np.meshgrid(\n",
    "    np.linspace(w1min, w1max, samples),\n",
    "    np.linspace(w2min, w2max, samples),\n",
    ")\n",
    "tl = train_loss_vec(w1, w2)\n",
    "\n",
    "ax.set_title(\"P茅rdida de entrenamiento promedio\")\n",
    "ax.set_xlabel(\"$w_1$\")\n",
    "ax.set_ylabel(\"$w_2$\")\n",
    "ax.set_zlabel(\"TrainLoss\")\n",
    "\n",
    "ax.plot_surface(\n",
    "    w1,\n",
    "    w2,\n",
    "    tl,\n",
    "    rstride=1,\n",
    "    cstride=1,\n",
    "    antialiased=False,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"none\",\n",
    "    linewidth=0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e82d22-6dd5-4545-abf1-870de8cf5922",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Algoritmo de optimizaci贸n</b>\n",
    "</center>\n",
    "\n",
    "Ahora discutimos la tercer decisi贸n de dise帽o, c贸mo podemos calcular el mejor predictor.\n",
    "\n",
    "Para responder esta pregunta, nos podemos olvidar de que estamos trabajando con regresi贸n lineal y aprendizaje m谩quina. Simplemente tenemos una funci贸n objetivo $\\mathrm{TrainLoss}(\\mathbf{w})$ que queremos minimizar.\n",
    "\n",
    "Comenzamos con una estimaci贸n de $\\mathbf{w}$ y mejoramos la estimaci贸n de forma iterativa para hacer que su valor de acuerdo a la funci贸n objetivo disminuya. Finalmente nos detenemos cuando la estimaci贸n sea suficientemente buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61d2e6-bc73-4de0-b9ad-4d81cf4502c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_weights():\n",
    "    return np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779a80f-fbb1-4a6b-a2b2-c7cac0095e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38aaa4-bd7a-46e9-be4b-de6655f0c4a3",
   "metadata": {},
   "source": [
    "Para lograr la mejora progresiva de la estimaci贸n consideramos el gradiente de la funci贸n objetivo, el cu谩l corresponde a un vector que apunta hacia d贸nde disminuye m谩s empinadamente la funci贸n objetivo.\n",
    "\n",
    "$$\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2c501-fa83-4684-87bf-18e2d50f0c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_train_loss(w):\n",
    "    total = sum(2 * (w.dot(phi(x)) - y) * phi(x) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf78d1-284c-4dae-819d-c99401032b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient_train_loss(np.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6383208-f7a8-4715-b833-f4fe7fa9cd4b",
   "metadata": {},
   "source": [
    "El proceso de optimizaci贸n iterativa es llamado **descenso de gradiente**, funciona de la siguiente manera:\n",
    "\n",
    "1. Inicializamos $\\mathbf{w}$ a alg煤n valor (p.ej. vector de ceros)\n",
    "2. Luego realizamos la siguiente actualizaci贸n $T$ veces, llamada la cantidad de 茅pocas (epochs en ingl茅s):\n",
    "3. Tomamos el vector $\\mathbf{w}$ y le restamos un valor escalado del gradiente, donde el factor de escalamiento (o tama帽o de paso) se denota $\\eta$, este par谩metro nos permite controlar qu茅 tan agresivo es el descenso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b8164-3366-4cd6-8457-2bcbef6ca171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(F, gradientF, init):\n",
    "    w = init()\n",
    "    eta = 0.1\n",
    "    for t in range(10):\n",
    "        value = F(w)\n",
    "        gradient = gradientF(w)\n",
    "        w = w - eta * gradient\n",
    "        print(f\"epoch {t}: w = {w}, F(w) = {value:.4f}, gradientF = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f2737-95fd-4265-a3dd-2719252e7f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient_descent(train_loss, gradient_train_loss, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9587b-dfd3-4a9d-a805-4ad4725f0313",
   "metadata": {},
   "source": [
    "驴C贸mo se minimiza la p茅rdida de entrenamiento promedio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa385dc2-9972-417a-8a91-8ff13fdd0b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent_viz(F, gradientF, init):\n",
    "    w = init()\n",
    "    w1s = [w[0]]\n",
    "    w2s = [w[1]]\n",
    "    tls = []\n",
    "    eta = 0.1\n",
    "    for t in range(100):\n",
    "        value = F(w)\n",
    "        tls.append(value)\n",
    "        gradient = gradientF(w)\n",
    "        w = w - eta * gradient\n",
    "        w1s.append(w[0])\n",
    "        w2s.append(w[1])\n",
    "    tls.append(F(np.array([w1s[-1], w2s[-1]])))\n",
    "    return np.array(w1s), np.array(w2s), np.array(tls)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "w1, w2 = np.meshgrid(\n",
    "    np.linspace(-1, 2, samples),\n",
    "    np.linspace(-1, 2, samples),\n",
    ")\n",
    "tl = train_loss_vec(w1, w2)\n",
    "\n",
    "ax.set_title(\"Minimizaci贸n de TrainLoss\")\n",
    "ax.set_xlabel(\"$w_1$\")\n",
    "ax.set_ylabel(\"$w_2$\")\n",
    "ax.set_zlabel(\"TrainLoss\")\n",
    "\n",
    "w1s, w2s, tls = gradient_descent_viz(train_loss, gradient_train_loss, initial_weights)\n",
    "\n",
    "ax.plot(w1s, w2s, tls, color=\"white\", alpha=0.5, zorder=10)\n",
    "\n",
    "ax.plot_surface(\n",
    "    w1,\n",
    "    w2,\n",
    "    tl,\n",
    "    rstride=1,\n",
    "    cstride=1,\n",
    "    antialiased=False,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolor=\"none\",\n",
    "    linewidth=0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f14260-6128-48e6-bb91-2281fb7bbdb8",
   "metadata": {},
   "source": [
    "## Repaso\n",
    "\n",
    "**驴Qu茅 predictores son posibles?**\n",
    "\n",
    "Construimos una *clase de hip贸tesis*.\n",
    "\n",
    "En lo que discutimos, consideramos *funciones lineales*,\n",
    "$$\\mathcal{F} = \\{ f_\\mathbf{w}(x) = \\mathbf{w}\\cdot\\phi(x) \\}$$\n",
    "$$\\phi(x) = [1, x]$$\n",
    "\n",
    "**驴Qu茅 tan bueno es un predictor?**\n",
    "\n",
    "Construimos una *funci贸n de p茅rdida*.\n",
    "\n",
    "En lo que discutimos, consideramos la *p茅rdida cuadr谩tica*,\n",
    "$$\\mathrm{Loss}(x, y, \\mathbf{w}) = (f_\\mathbf{w}(x) - y)^2$$\n",
    "\n",
    "**驴C贸mo calcular el mejor predictor?**\n",
    "\n",
    "Construimos un *algoritmo de optimizaci贸n*.\n",
    "\n",
    "En lo que discutimos, consideramos el *descenso de gradiente*,\n",
    "$$\\mathbf{w} \\gets \\mathbf{w} - \\eta\\nabla\\mathrm{TrainLoss}(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f05093-5078-497b-8d74-144ec112c70e",
   "metadata": {},
   "source": [
    "# Clasificaci贸n lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c60741-e5b2-4145-90eb-142adb216d6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Al igual que en la regresi贸n lineal, recolectamos o se nos provee con **datos de entrenamiento**, ($\\mathcal{D}_\\mathrm{train}$), pero ahora estos datos consisten de **ejemplos** con dos entradas reales $(x_1, x_2)$ y una salida binaria $y$ ($+1$ o $-1$).\n",
    "\n",
    "<center>\n",
    "    <img src=\"./classification-framework.png\" />\n",
    "</center>\n",
    "\n",
    "Tambi茅n vamos a querer construir un algoritmo de aprendizaje que tome los datos de entrenamiento y produzca un modelo $f$, al que llamamos **clasificador**. Estos modelos nos van a permitir clasificar nuevas entradas nunca antes vistas durante el entrenamiento. En el diagrama de arriba, se alimenta la entrada $[2, 0]$ al clasificador para obtener $f([2, 0])$ (en este ejemplo $-1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a5015-4054-4262-aa1a-b999018377e6",
   "metadata": {},
   "source": [
    "En Python podemos definir los datos de entrenamiento como una lista de parejas $(x, y)$, donde $x$ se representa como una tupla de dos elementos $(x_1, x_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3c496-e57a-4ca4-87d2-84ba73887c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    ((0, 2), 1),\n",
    "    ((-2, 0), 1),\n",
    "    ((1, -1), -1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17bbe5-aa70-4e05-9f47-a35f9e8794b7",
   "metadata": {},
   "source": [
    "Supongamos que nuestro clasificador es muy simple y corresponde a una relaci贸n lineal entre la primera y segunda componente de las entradas.\n",
    "\n",
    "Por el momento, usamos la recta identidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb99409-6911-4c10-af1f-70f3e2cf6e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x1):\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15dfac2-2aa1-41df-a717-968655eaef91",
   "metadata": {},
   "source": [
    "Para visualizar los datos de entrenamiento, el modelo y la clasificaci贸n de una nueva entrada $[2,0]$ podemos graficar $x_1$ contra $x_2$ en el plano cartesiano. El modelo entonces parte el espacio de entrada en dos, uno donde se clasifican los puntos con $+1$ y otro donde se clasifican con $-1$.\n",
    "\n",
    "A la frontera entre estas dos divisiones se le llama **frontera de decisi贸n**. Graficamos una flecha perpendicular a la frontera para indicar qu茅 regi贸n es la positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124a26e-5e5c-45f9-acf5-44e1eaa719cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs1plus = [x1 for (x1, x2), y in train_examples if y == +1]\n",
    "xs2plus = [x2 for (x1, x2), y in train_examples if y == +1]\n",
    "\n",
    "xs1minus = [x1 for (x1, x2), y in train_examples if y == -1]\n",
    "xs2minus = [x2 for (x1, x2), y in train_examples if y == -1]\n",
    "\n",
    "xfs = np.linspace(-3, 3)\n",
    "yfs = f(xfs)\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.xaxis.set_ticks(range(-3, 4))\n",
    "ax.yaxis.set_ticks(range(-3, 4))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs1plus, xs2plus, c=\"C0\", zorder=1, label=\"$+1$\")\n",
    "ax.scatter(xs1minus, xs2minus, c=\"C2\", zorder=1, label=\"$-1$\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2)\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0, 0),\n",
    "    xytext=(-0.6, +0.6),\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<-\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    ")\n",
    "ax.scatter(2, 0, c=\"white\", edgecolors=\"C2\", zorder=3, linewidth=2)\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a1010-8127-481f-ad1a-1a91cdad4946",
   "metadata": {},
   "source": [
    "Retomamos las mismas tres decisiones de dise帽o para nuestro algoritmo de aprendizaje.\n",
    "\n",
    "1. 驴Qu茅 clasificadores $f$ se le permite al algoritmo producir? 驴La frontera de decisi贸n debe ser recta o puede curvarse? En otras palabras, 驴Cu谩l es la **clase de hip贸tesis**?\n",
    "2. 驴C贸mo juzga el algoritmo de aprendizaje cu谩l clasificador es bueno? En otras palabras, 驴Cu谩l es la **funci贸n de p茅rdida**?\n",
    "3. 驴C贸mo el algoritmo de aprendizaje realmente encuentra el mejor clasificador? En otras palabras, 驴Cu谩l es el **algoritmo de optimizaci贸n**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fbd5d-6ae6-431b-9d0a-0705ad46bf54",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Clase de hip贸tesis</b>\n",
    "</center>\n",
    "\n",
    "\n",
    "Vamos a considerar todos los clasificadores de la forma\n",
    "\n",
    "$$f(x_1, x_2) = \\mathrm{sign}(w_1x_1 + w_2x_2)$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\\mathrm{sign}(z) = \\begin{cases}\n",
    "+1 &\\text{si } z > 0 \\\\\n",
    "-1 &\\text{si } z < 0 \\\\\n",
    "0  &\\text{si } z = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Donde la frontera de decisi贸n consiste de todos los puntos $(x_1, x_2)$ tal que $w_1x_1+w_2x_2 = 0$.\n",
    "\n",
    "Intentemos entender mejor lo que esta clase de hip贸tesis representa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05341dd9-a496-42c9-876d-382ea85b0dca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550b48f-d827-406c-aa57-f12987819569",
   "metadata": {},
   "source": [
    "Al igual que en el caso de la regresi贸n lineal, planteamos la clase de hip贸tesis utilizando notaci贸n vectorial.\n",
    "\n",
    "La clasificaci贸n lineal se parametriza por un vector de pesos $\\mathbf{w}$.\n",
    "\n",
    "$$\\mathbf{w} = [w_1, w_2]$$\n",
    "\n",
    "Definimos un extractor de caracter铆sticas $\\phi$ como la identidad.\n",
    "\n",
    "$$\\phi(x) = [x_1, x_2]$$\n",
    "\n",
    "Y procedemos a definir el clasificador $f_\\mathbf{w}$ que toma $x$ de entrada y calcula el signo del producto punto entre el vector de pesos $\\mathbf{w}$ y el vector de caracter铆sticas $\\phi(x)$.\n",
    "\n",
    "$$f_\\mathbf{w}(x) = \\mathrm{sign}(\\mathbf{w}\\cdot\\phi(x))$$\n",
    "\n",
    "Podemos encontrar un sentido geom茅trico para estos clasificadores.\n",
    "\n",
    "- Considerando $\\mathbf{w}$ y $\\phi(x)$ como vectores, su producto punto es un factor positivo multiplicado por el coseno del 谩ngulo $\\theta$ entre ellos. Si $\\theta < \\pi/2$, entonces $\\cos(\\theta)$ es positivo y el signo en $f_\\mathbf{w}$ ignora las magnitudes. Cuando los vectores son perpendiculares, es porque $\\phi(x)$ se encuentra justo sobre la frontera de decisi贸n. \n",
    "- Al graficar los ejemplos como puntos en tres dimensiones (la etiqueta siendo la tercera dimensi贸n) el clasificador $f_\\mathbf{w}$ corresponde a un plano en el espacio que siempre corta al origen. Todos los puntos por arriba del plano se clasifican positivos y todos por debajo se clasifican negativos.\n",
    "\n",
    "Finalmente, definimos la clase de hip贸tesis $\\mathcal{F}$ como el conjunto de todos los posibles predictores $f_{\\mathbf{w}}$.\n",
    "\n",
    "$$\\mathcal{F} = \\left\\{ f_{\\mathbf{w}} : \\mathbf{w} \\in \\mathbb{R}^2 \\right\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f276c-4341-449f-9657-3c242b31a743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7c8ab-8285-4f1e-872d-2fbaa7f4decd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sign(z):\n",
    "    if z > 0:\n",
    "        return +1\n",
    "    if z < 0:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ec06d-d311-4d2b-b4c6-2936d066cba7",
   "metadata": {},
   "source": [
    "Consideremos el clasificador con $\\mathbf{w} = [0.5, 1]$, 驴Qu茅 tal clasifica los datos de entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf6ffa-b1ff-45a0-a6f0-00051f507ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = np.array([0.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b954b5-b182-4f06-82ae-6234222eb6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs1plus = [x1 for (x1, x2), y in train_examples if y == +1]\n",
    "xs2plus = [x2 for (x1, x2), y in train_examples if y == +1]\n",
    "\n",
    "xs1minus = [x1 for (x1, x2), y in train_examples if y == -1]\n",
    "xs2minus = [x2 for (x1, x2), y in train_examples if y == -1]\n",
    "\n",
    "xfs = np.linspace(-3, 3)\n",
    "yfs = -(w[0] / w[1]) * xfs\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.xaxis.set_ticks(range(-3, 4))\n",
    "ax.yaxis.set_ticks(range(-3, 4))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_title(\"Predicci贸n con clasificador lineal\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs1plus, xs2plus, c=\"C0\", zorder=1, label=\"$+1$\")\n",
    "ax.scatter(xs1minus, xs2minus, c=\"C2\", zorder=1, label=\"$-1$\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2)\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0, 0),\n",
    "    xytext=(w[0], w[1]),\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<-\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea944ee0-ac9a-4af9-bfd4-9988874e621c",
   "metadata": {},
   "source": [
    "Vemos que dos ejemplos los clasifica correctamente, pero un tercero con etiqueta positiva es clasificado como negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f866333-8a1e-4759-9436-e1d40f2262d1",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Funci贸n de p茅rdida</b>\n",
    "</center>\n",
    "\n",
    "Utilizamos una funci贸n de p茅rdida llamada **cero-uno** que regresa cero cuando la etiqueta del ejemplo y el resultado de la clasificaci贸n coinciden, y que regresa uno cuando difieren.\n",
    "\n",
    "$$\\mathrm{Loss}_{0-1}(x, y, \\mathbf{w}) = \\mathbf{1}[f_\\mathbf{w}(x) \\not= y]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd275b-c3ba-40ac-8a44-2dfbcaf093db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return int(sign(phi(x).dot(w)) != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5a6ad-5823-4bef-b173-5106292ee408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[0], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b788e-cb48-419b-8ae6-120f138fb854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[1], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8eb0e2-5dc0-4bfc-a2e1-942ccec0c632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[2], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8230c-00a4-4b1b-b1ce-24793748c252",
   "metadata": {},
   "source": [
    "$$f_\\mathbf{w}(x) = \\mathrm{sign}(\\mathbf{w}\\cdot \\phi(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9594f-8ff2-4772-aece-580dcc76936a",
   "metadata": {},
   "source": [
    "La p茅rdida total es simplemente el promedio de la p茅rdida cero-uno sobre todos los ejemplos de entrenamiento, en este ejemplo ser铆a $\\frac{1}{3}$.\n",
    "\n",
    "Antes de discutir la tercer decisi贸n de dise帽o (el algoritmo de optimizaci贸n), veamos otras funciones de p茅rdida que pueden resultar convenientes de utilizar.\n",
    "\n",
    "Recordemos que cuando $\\mathbf{w}\\cdot\\phi(x)$ es positiva, la etiqueta predecida es $+1$ debido a que el 谩ngulo interno entre los dos vectores es menor a $90^\\circ$.\n",
    "\n",
    "La magnitud absoluta de este escalar es proporcional a la distancia entre el ejemplo de entrenamiento y la frontera de decisi贸n. Por lo tanto, podemos considerar este escalar como una m茅trica de qu茅 tan seguros estamos en predecir $+1$. Si la distancia entre un ejemplo y la frontera de decisi贸n es peque帽a, entonces peque帽os cambios en el modelo pueden producir cambios en la predicci贸n. Si la distancia es muy grande, entonces el modelo pudiera cambiar sin alterar la predicci贸n.\n",
    "\n",
    "Podemos adecuar este concepto utilizando el **margen**, el cu谩l representa qu茅 tan correcta es la predicci贸n. Basta multiplicar $\\mathbf{w}\\cdot\\phi(x)$ por la etiqueta de los datos de entrenamiento asociada a $x$. Con esta modificaci贸n, un margen negativo indica errores de predicci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407400ce-1818-4865-a57e-76c4a19c4250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x, y in train_examples:\n",
    "    x = np.array(x)\n",
    "    print(f\"phi(x)={x}, margen={w.dot(phi(x))*y}\")\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "xs1plus = [x1 for (x1, x2), y in train_examples if y == +1]\n",
    "xs2plus = [x2 for (x1, x2), y in train_examples if y == +1]\n",
    "\n",
    "xs1minus = [x1 for (x1, x2), y in train_examples if y == -1]\n",
    "xs2minus = [x2 for (x1, x2), y in train_examples if y == -1]\n",
    "\n",
    "xfs = np.linspace(-3, 3)\n",
    "yfs = -(w[0] / w[1]) * xfs\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.xaxis.set_ticks(range(-3, 4))\n",
    "ax.yaxis.set_ticks(range(-3, 4))\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_title(\"Predicci贸n con clasificador lineal\")\n",
    "\n",
    "ax.grid(axis=\"y\")\n",
    "ax.scatter(xs1plus, xs2plus, c=\"C0\", zorder=1, label=\"$+1$\")\n",
    "ax.scatter(xs1minus, xs2minus, c=\"C2\", zorder=1, label=\"$-1$\")\n",
    "ax.plot(xfs, yfs, \":\", c=\"black\", zorder=2)\n",
    "ax.annotate(\n",
    "    \"\",\n",
    "    xy=(0, 0),\n",
    "    xytext=(w[0], w[1]),\n",
    "    arrowprops={\n",
    "        \"arrowstyle\": \"<-\",\n",
    "        \"linestyle\": \":\",\n",
    "    },\n",
    ")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59fc5bc-aa99-4275-bb80-cb0047d6307a",
   "metadata": {},
   "source": [
    "Podemos reescribir la p茅rdida cero-uno en t茅rminos del margen, recordemos que el margen es positivo cuando la clasificaci贸n es correcta.\n",
    "\n",
    "$$\\mathrm{Loss}_\\text{0-1}(x, y, \\mathbf{w}) = \\mathbf{1}[f_\\mathbf{w}(x) \\not= y]$$\n",
    "\n",
    "$$\\mathrm{Loss}_\\text{0-1}(x, y, \\mathbf{w}) = \\mathbf{1}[\\underbrace{(\\mathbf{w}\\cdot\\phi(x))y}_\\text{margen} \\leq 0]$$\n",
    "\n",
    "Grafiquemos el margen contra la p茅rdida cero uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d80800-25eb-4c22-ab21-4c337f82194c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ms = np.linspace(-3, 3, 200)\n",
    "ls = 1.0 * (ms <= 0.0)\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.set_xlabel(\"margen $(\\mathbf{w}\\cdot\\phi(x))y$\")\n",
    "ax.set_ylabel(\"$\\mathrm{Loss}(x, y, \\mathbf{w})$\")\n",
    "\n",
    "ax.plot(ms, ls);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526a6ad-4428-4884-bb5e-116d29cb5327",
   "metadata": {},
   "source": [
    "Una desventaja clara con utilizar la p茅rdida cero-uno es que tiene gradientes cero, esto significa que utilizar un algoritmo de optimizaci贸n como el descenso de gradiente, f谩cilmente caemos en un m铆nimo local.\n",
    "\n",
    "Conceptualmente, quisieramos que cuando el margen sea negativo, la p茅rdida sea mayor a cero, mientras que si el margen es positivo, la p茅rdida sea cercana a cero. Podemos tomar p茅rdidas m谩s grandes entre mas negativo sea el margen y tender a cero a partir de que el margen sea positivo.\n",
    "\n",
    "Discutamos dos funciones de p茅rdida alternativas que capturan el esp铆ritu de lo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877652f-2f59-4aeb-8889-54358c8a3ad8",
   "metadata": {},
   "source": [
    "La p茅rdida de articulaci贸n (*Hinge loss*) penaliza linealmente los m谩rgenes no-positivos y considera una p茅rdida de cero para los m谩rgenes positivos.\n",
    "\n",
    "Una forma usual de la p茅rdida de articulaci贸n es:\n",
    "\n",
    "$$\\mathrm{Loss}_\\mathrm{hinge}(x, y, \\mathbf{w}) = \\max\\{1 - (\\mathbf{w}\\cdot\\phi(x))y, 0\\}$$\n",
    "\n",
    "El $1$ en la relaci贸n lineal nos permite tener un poco de holgura, queremos que el clasificador no solo sea correcto, si no que clasifique con un margen positivo. Un algoritmo de aprendizaje intentar铆a encontrar un clasificador que produzca margenes m谩s grandes a pesar de haber encontrado un clasificador correcto.\n",
    "\n",
    "Visualicemos c贸mo se comporta la p茅rdida conforme cambia el margen de acuerdo a las dos p茅rdidas que hemos visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0757a82-b1d1-4c85-90ae-e2120c323f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ms = np.linspace(-3, 3, 200)\n",
    "ls1 = 1.0 * (ms <= 0.0)\n",
    "ls2 = ((1.0 - ms) > 0) * (1.0 - ms)\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.set_xlabel(\"margen $(\\mathbf{w}\\cdot\\phi(x))y$\")\n",
    "ax.set_ylabel(\"$\\mathrm{Loss}(x, y, \\mathbf{w})$\")\n",
    "\n",
    "ax.plot(ms, ls1, label=\"$\\mathrm{Loss}_\\mathrm{0-1}$\")\n",
    "ax.plot(ms, ls2, label=\"$\\mathrm{Loss}_\\mathrm{hinge}$\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63419e-cac1-4da4-802a-0653c6fb9fd4",
   "metadata": {},
   "source": [
    "Observemos que la p茅rdida de articulaci贸n acota por arriba la p茅rdida cero-uno, tambi茅n son exactamente iguales para m谩rgenes mayores a uno.\n",
    "\n",
    "Cuando reducimos el valor de la p茅rdida de articulaci贸n, tambi茅n reducimos en general la p茅rdida cero-uno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b482f53-f11f-45b0-8d07-b34dc4ea493c",
   "metadata": {},
   "source": [
    "La p茅rdida log铆stica (*Logistic loss*) es otra alternativa, la idea es intentar aumentar el margen aunque ya sea mayor a uno.\n",
    "\n",
    "$$\\mathrm{Loss}_\\mathrm{logistic}(x, y, \\mathbf{w}) = \\ln{\\left(1 + e^{-(\\mathbf{w}\\cdot\\phi(x))y}\\right)}$$\n",
    "\n",
    "Visualicemos las tres p茅rdidas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f3d70-80fe-4b22-acb7-1aae4fd4cd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ms = np.linspace(-3, 3, 200)\n",
    "ls1 = 1.0 * (ms <= 0.0)\n",
    "ls2 = ((1.0 - ms) > 0) * (1.0 - ms)\n",
    "ls3 = np.log(1 + np.exp(-ms))\n",
    "\n",
    "ax.set_xlim([-3, 3])\n",
    "ax.set_ylim([0, 4])\n",
    "ax.set_xlabel(\"margen $(\\mathbf{w}\\cdot\\phi(x))y$\")\n",
    "ax.set_ylabel(\"$\\mathrm{Loss}(x, y, \\mathbf{w})$\")\n",
    "\n",
    "ax.plot(ms, ls1, label=\"$\\mathrm{Loss}_\\mathrm{0-1}$\")\n",
    "ax.plot(ms, ls2, label=\"$\\mathrm{Loss}_\\mathrm{hinge}$\")\n",
    "ax.plot(ms, ls3, label=\"$\\mathrm{Loss}_\\mathrm{logistic}$\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135384b-b76a-4292-a094-bc160b93fd48",
   "metadata": {},
   "source": [
    "Elegimos la p茅rdida de articulaci贸n para lo que resta de la clase.\n",
    "\n",
    "驴C贸mo podemos implementarla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b479e6-91bf-49fa-bf30-dd18fe44f65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(x, y, w):\n",
    "    return max(1 - phi(x).dot(w) * y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ac1ee-4e25-40e7-9bab-04b0e72b4b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20eba1-801f-465c-8700-fe2333b06428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b6327-be40-4739-9d21-8b9ce283b37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[0], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3f157-f468-4c89-b3d5-88e868522834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[1], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2d571-098b-40d6-97dc-c45c9bd0c288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss(*train_examples[2], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a92f3d-040c-4cda-ab69-84ad5161388a",
   "metadata": {},
   "source": [
    "Calculemos el gradiente de la p茅rdida de articulaci贸n, consideramos dos casos, cuando el m谩rgen es menor a uno y cuando no.\n",
    "\n",
    "$$\\mathrm{Loss}_\\mathrm{hinge}(x, y, \\mathbf{w}) = \\max\\{1 - (\\mathbf{w}\\cdot\\phi(x))y, 0\\}$$\n",
    "\n",
    "$$\\nabla\\mathrm{Loss}_\\mathrm{hinge}(x, y, \\mathbf{w}) = \\begin{cases}\n",
    "-\\phi(x)y &\\text{si}\\ 1-(\\mathbf{w}\\cdot\\phi(x))y > 0\\\\\n",
    "0 &\\text{en otro caso}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8583ea-4422-4239-9e19-9e6221846075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum(max(1 - w.dot(phi(x)) * y, 0) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902222dd-fd32-44d3-bf25-2e802701bf89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78e80a-d4d5-42ba-94da-a5e4830b10d4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Algoritmo de optimizaci贸n</b>\n",
    "</center>\n",
    "\n",
    "隆Utilicemos el descenso de gradiente!\n",
    "\n",
    "Una vez que definimos la p茅rdida de entrenamiento utilizando la p茅rdida de articulaci贸n, solo tenemos que implementar el gradiente de esta nueva p茅rdida de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a4e62-013d-442f-81b3-e6d0756f2ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_train_loss(w):\n",
    "    total = sum(\n",
    "        -phi(x) * y if 1 - w.dot(phi(x)) * y > 0 else 0 for x, y in train_examples\n",
    "    )\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a854290-a652-4312-9529-f72f2f60c101",
   "metadata": {},
   "source": [
    "La implementaci贸n de los pesos iniciales y el algoritmo de descenso de gradiente pueden quedar intactos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd6582-77c4-4e57-bb39-e5c0a864ec48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradient_descent(train_loss, gradient_train_loss, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eef779-3d02-46a9-9704-4245b52a6739",
   "metadata": {},
   "source": [
    "## Repaso\n",
    "\n",
    "**驴Qu茅 predictores son posibles?**\n",
    "\n",
    "Construimos una *clase de hip贸tesis*.\n",
    "\n",
    "En lo que discutimos, consideramos *fontera de decisi贸n lineal*.\n",
    "\n",
    "**驴Qu茅 tan bueno es un predictor?**\n",
    "\n",
    "Construimos una *funci贸n de p茅rdida*.\n",
    "\n",
    "Consideramos la p茅rdida cero-uno, de articulaci贸n y log铆stica.\n",
    "\n",
    "**驴C贸mo calcular el mejor predictor?**\n",
    "\n",
    "Construimos un *algoritmo de optimizaci贸n*.\n",
    "\n",
    "Usamos *descenso de gradiente*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bd9b6-c1a8-447e-a171-9defef651913",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Descenso de gradiente estoc谩stico\n",
    "\n",
    "En los dos tipos de problemas que discutimos en clases pasadas, utilizamos el descenso de gradiente, un algoritmo de prop贸sito general para optimizar la p茅rdida de entrenamiento.\n",
    "\n",
    "$$\\mathrm{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(x, y, \\mathbf{w})$$\n",
    "\n",
    "*Descenso de gradiente:*\n",
    "1. Inicializamos $\\mathbf{w}$ a alg煤n valor\n",
    "2. Luego realizamos la siguiente actualizaci贸n $T$ veces:\n",
    "3. $\\mathbf{w} \\gets \\mathbf{w} - \\eta\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w})$\n",
    "\n",
    "\n",
    "Sin embargo, este algoritmo es muy lento, la p茅rdida de entrenamiento realiza una sumatoria sobre todos los datos de entrenamiento. Si tenemos millones de ejemplos, entonces cada c谩lculo del gradiente requiere considerar cada uno de estos millones de ejemplos. Todo esto antes de ajustar el modelo aunque sea poquito.\n",
    "\n",
    "驴Podemos ir mejorando el modelo antes de procesar todos los ejemplos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53ba9a-59ca-4a78-8dc6-1fe531f1a9d7",
   "metadata": {},
   "source": [
    "La respuesta es el descenso de gradiente estoc谩stico (*SGD* por *Stochastic Gradient Descent*)\n",
    "\n",
    "En lugar de iterar sobre todos los ejemplos para calcular un gradiente y dar un paso, el SGD itera sobre los ejemplos y actualiza los pesos en cada ejemplo.\n",
    "\n",
    "Cada actualizaci贸n no es tan buena ya que la actualizaci贸n del modelo es en base a un solo ejemplo, sin embargo, podemos hacer m谩s actualizaciones de esta forma.\n",
    "\n",
    "*Descenso de gradiente estoc谩stico:*\n",
    "1. Inicializamos $\\mathbf{w}$ a alg煤n valor\n",
    "2. Luego realizamos lo siguiente $T$ veces\n",
    "3. Para cada ejemplo $(x, y)$ en $\\mathcal{D}_\\mathrm{train}$:\n",
    "4. $\\mathbf{w}\\gets\\mathbf{w}-\\eta\\nabla_\\mathbf{w}\\mathrm{Loss}(x, y, \\mathbf{w})$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./sgd.png\" />\n",
    "</center>\n",
    "\n",
    "驴Prefieren *calidad* o *cantidad*? El SGD prefiere cantidad\n",
    "\n",
    "Solo hay un detalle... Elegir el tama帽o de paso puede ser complicado.\n",
    "\n",
    "Entre m谩s grande sea $\\eta$ m谩s agresivo es el descenso, podemos llegar al m铆nimo m谩s r谩pido, pero tambi茅n enfierrarnos de m谩s y nunca llegar a 茅l.\n",
    "\n",
    "Si elegimos un $\\eta$ muy peque帽o, las actualizaciones ser铆an mas estables, pero llegar铆amos al m铆nimo demasiado lento. En el caso extremo cuando $\\eta = 0$ el modelo nunca es actualizado.\n",
    "\n",
    "Una forma de mitigar esto es variar $\\eta$ en cada actualizaci贸n del modelo. En particular, vamos a considerar que iniciamos con $\\eta=1$ y luego ajustarla en funci贸n a la cantidad de actualizaciones realizadas hasta el momento $u$,\n",
    "$$\\eta \\gets \\frac{1}{\\sqrt{u}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02da2c-a2a7-4f7a-83a7-059575700bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "xs = np.linspace(1, 10, 100)\n",
    "ys = 1.0 / np.sqrt(xs)\n",
    "\n",
    "ax.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d576312-754f-40a4-b9c3-2ec0a86cefe7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450636a-5a61-4139-97cf-746624ca065e",
   "metadata": {},
   "source": [
    "Consideremos el problema de regresi贸n lineal, pero utilizando datos de entrenamiento aleatorios.\n",
    "\n",
    "Vamos a considerar un modelo lineal para generar los datos, representado por un vector de cinco componentes (pensemos en un hiperplano en cinco dimensiones).\n",
    "\n",
    "Este ser谩 nuestro vector de pesos \"real\", pero ni la p茅rdida ni el SGD sabr谩n que valores tiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e3e1b9-b78b-4344-81e4-424843ab0f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trueW = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8f5f5-5c3b-4b9f-8419-c9ee754f092d",
   "metadata": {},
   "source": [
    "Generamos ejemplos donde la entrada $x$ es un vector aleatorio donde cada componente sigue la distribuci贸n normal. Luego obtenemos la salida que predice el vector de persos real y le agregamos ruido, tambi茅n con distribuci贸n normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733940f4-6c4e-40f0-8ebe-325415e3a595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = np.random.randn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d73f3b-884d-4dd9-89fd-da68435928cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ecf9b-aaf5-4467-86cb-9a4242e18bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.hist(v, bins=100, density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88828f48-2849-4966-a76d-97a938dc05d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    x = np.random.randn(len(trueW))\n",
    "    y = trueW.dot(x) + np.random.normal(0, 2)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b570-b876-473a-b02d-9172b9216f2c",
   "metadata": {},
   "source": [
    "Generemos los datos de entrenamiento con un mill贸n de ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63045236-4f04-4b84-a282-69bc6fbec52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6ea4c-cc25-4855-b681-21f8f735d732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [generate() for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3ab29-240f-481f-9c33-17ed141444d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa76096-8acd-4bff-8afa-e87154afde5d",
   "metadata": {},
   "source": [
    "Recordemos c贸mo modelamos en programas de Python la regresi贸n lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed6bc4-09c7-4ff9-85ec-6abe10608434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4ec1f-4237-4f28-85ab-5ccf889bb313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_weights():\n",
    "    return np.zeros(len(trueW))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7e5ff-bcde-4983-84c9-f8a11ac4d327",
   "metadata": {},
   "source": [
    "Para descenso de gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a769a8-82f1-464d-8dfa-cfcdb635c174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum((w.dot(phi(x)) - y) ** 2 for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e613c-89ea-47d7-bfae-a1a0e10516dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_train_loss(w):\n",
    "    total = sum(2 * (w.dot(phi(x)) - y) * phi(x) for x, y in train_examples)\n",
    "    average = 1.0 / len(train_examples) * total\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7d611-b501-491a-bee9-635e0d31107d",
   "metadata": {},
   "source": [
    "Para descenso de gradiente estoc谩stico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad6e70-7942-42fc-93ee-34733ebe6239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(w, i):\n",
    "    x, y = train_examples[i]\n",
    "    return (w.dot(phi(x)) - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf887d-ac3a-43e7-b41b-373e01ec6f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_loss(w, i):\n",
    "    x, y = train_examples[i]\n",
    "    return 2 * (w.dot(phi(x)) - y) * phi(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34571d-4c99-4523-8c89-1d8f1aa76cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GDepochs = 10\n",
    "SGDepochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd10b3f-5220-4724-9010-86d23e6a6da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradientDescent(F, gradientF, init):\n",
    "    w = init()\n",
    "    eta = 0.1\n",
    "    for t in range(GDepochs):\n",
    "        value = F(w)\n",
    "        gradient = gradientF(w)\n",
    "        w = w - eta * gradient\n",
    "        print(f\"epoch {t}: w = {w}, F(w) = {value:.4f}, gradientF = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6c21a-6a13-4061-add1-2dd9cd8d0820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stochasticGradientDescent(f, gradientf, n, init):\n",
    "    w = init()\n",
    "    numUpdates = 0\n",
    "    for t in range(SGDepochs):\n",
    "        for i in range(n):\n",
    "            value = f(w, i)\n",
    "            gradient = gradientf(w, i)\n",
    "            numUpdates += 1\n",
    "            eta = 1.0 / np.sqrt(numUpdates)\n",
    "            w = w - eta * gradient\n",
    "        print(f\"epoch {t}: w = {w}, F(w) = {value:.4f}, gradientF = {gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b2d83-d571-4b64-95ea-8c4115185731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "gradientDescent(train_loss, gradient_train_loss, initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb04b41-a186-4ad2-995a-74dcafd7ea0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -r1 -n1\n",
    "stochasticGradientDescent(loss, gradient_loss, len(train_examples), initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cdd89-a41d-4240-83dd-13ee2859eef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss(np.array([0.98478211, 2.02829034, 2.98268202, 4.05039687, 5.0458254]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede8753-4265-4fb6-a010-cf1a90e44557",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Optimizaci贸n de grupo distribucionalmente robusta\n",
    "\n",
    "Tambi茅n llamada *group DRO* por *group distributionally robust optimization*.\n",
    "\n",
    "---\n",
    "\n",
    "Hasta ahora nos hemos enfocado en encontrar modelos que minimizan la p茅rdida de entrenamiento, la cu谩l es el promedio de la p茅rdida sobre los ejemplos de entrenamiento.\n",
    "\n",
    "Considerar el promedio puede parecer intuitivo y quiz谩 lo correcto... Veamos por qu茅 esto puede nos er el caso...\n",
    "\n",
    "Ver video: http://gendershades.org/\n",
    "\n",
    "El proyecto *Gender Shades* es solo uno de los muchos ejemplos que apuntan a un problema general y sist茅mico: los modelos de aprendizaje m谩quina, t铆picamente optimizados para maximizar la precisi贸n promedio, pueden resultar en precisiones pobres para ciertos **grupos** (subpoblaciones).\n",
    "\n",
    "Estos grupos pueden ser definidos por clases protegidas. De acuerdo a la CNDH algunos temas relativos a grupos vulnerables en M茅xico son:\n",
    "- Personas Migrantes\n",
    "- Personas en Situaci贸n de V铆ctimas\n",
    "- Ni帽as, Ni帽os y Adolescentes\n",
    "- J贸venes, Personas Mayores y Familias\n",
    "- Personas desaparecidas y no localizadas\n",
    "- Sexualidad, Salud y VIH\n",
    "- Igualdad entre Mujeres y Hombres\n",
    "- Periodistas y Personas Defensoras Civiles\n",
    "- Contra la Trata de Personas\n",
    "- Pueblos y Comunidades Ind铆genas y Afrodescendientes\n",
    "- Personas con Discapacidad\n",
    "- Prevenci贸n de la Tortura\n",
    "- Personas Privadas de la Libertad\n",
    "- Derechos Econ贸micos, Sociales, Culturales y Ambientales\n",
    "\n",
    "(Ver http://informe.cndh.org.mx/menu.aspx?id=80002)\n",
    "\n",
    "Puedes consultar informaci贸n sobre el trabajo legislativo de nuestros representantes que constituyen la comisi贸n de *Atenci贸n a Grupos Vulnerables* en la *C谩mara de Diputados*: https://web.diputados.gob.mx/inicio/comision?Oid=f8cfe362-3718-4e81-b0f6-9a85f0a12ecc\n",
    "\n",
    "En un caso particular, un hombre negro con el nombre de Robert Julian-Borchak Williams fue arrestado por error debido a una coincidencia incorrecta con otro hombre negro capturado en un video de vigilancia. Este error fue cometido por un sistema de reconocimiento facial.\n",
    "\n",
    "(Ver https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html)\n",
    "\n",
    "Con el proyecto Gender Shades, podemos ver que precisiones m谩s bajas para algunos grupos pueden tener consecuencias en la vida real, como m谩s arrestos err贸neos, lo cu谩l agravia a煤n m谩s las desigualdades que existen actualmente en nuestra sociedad.\n",
    "\n",
    "Vamos a enfocarnos en las disparidades de rendimiento en el aprendizaje m谩quina y c贸mo podemos mitigarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10331c5-c2ab-44c1-969f-92013aa25be4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b>Regresi贸n lineal con grupos</b>\n",
    "</center>\n",
    "\n",
    "Consideremos la regresi贸n lineal.\n",
    "\n",
    "Comenzamos con datos de entrenamiento, pero ahora cada ejemplo consiste no solo de la entrada $x$ y la salida $y$, si no tambi茅n de un **grupo** $g$.\n",
    "\n",
    "En este ejemplo, suponemos que tenemos dos grupos $A$ y $B$.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./dro00.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95add0-e89a-473a-92b1-4705d7095646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    (1, 4, \"A\"),\n",
    "    (2, 8, \"A\"),\n",
    "    (5, 5, \"B\"),\n",
    "    (6, 6, \"B\"),\n",
    "    (7, 7, \"B\"),\n",
    "    (8, 8, \"B\"),\n",
    "]\n",
    "\n",
    "pred_x = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63cc3a-c56f-45f6-bb56-45812e8a70d4",
   "metadata": {},
   "source": [
    "$$\\phi(x) = [x]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dd668-1827-423d-8c5f-baac434ddbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.array([x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa982f6a-13cf-4781-9b7f-fc60d67d2aea",
   "metadata": {},
   "source": [
    "$$f_\\mathbf{w}(x) = \\mathbf{w}\\cdot\\phi(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c4486-85eb-4882-ade3-75b5a6582695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    return np.dot(w, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000ee2-e3e7-40a1-a0a7-939a3de489eb",
   "metadata": {},
   "source": [
    "$$\\mathrm{Loss}(\\mathbf{w}, x, y) = (f_\\mathbf{w}(x)-y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc0a13-13d3-4503-bba2-573a42d85792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return (predict(w, x) - y) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b881cc-73a3-4bf2-aeca-aef6117811ad",
   "metadata": {},
   "source": [
    "$$\\mathrm{TrainLoss}(\\mathbf{w}) = \\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y,g)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(\\mathbf{w}, x, y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71e26-a378-49be-9aa9-808b29016169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss(w):\n",
    "    total = sum(loss(w, x, y) for x, y, g in train_examples)\n",
    "    return total / len(train_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc2cb7-8e1c-4fb5-b299-761550e954ec",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\nabla_\\mathbf{w}\\mathrm{Loss}(\\mathbf{w}, x, y) &= \\nabla_\\mathbf{w} (f_\\mathbf{w}(x)-y)^2 \\\\\n",
    "                                                 &= \\nabla_\\mathbf{w} ((\\mathbf{w}\\cdot\\phi(x))-y)^2 \\\\\n",
    "                                                 &= 2\\left((\\mathbf{w}\\cdot\\phi(x))-y\\right)\\nabla_\\mathbf{w}\\left((\\mathbf{w}\\cdot\\phi(x))-y\\right)  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\nabla_\\mathbf{w}\\left((\\mathbf{w}\\cdot\\phi(x))-y\\right)  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\left( \\nabla_\\mathbf{w}(\\mathbf{w}\\cdot\\phi(x)) - \\nabla_\\mathbf{w}y\\right)  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\nabla_\\mathbf{w}(\\mathbf{w}\\cdot\\phi(x))  \\\\\n",
    "                                                 &= 2(f_\\mathbf{w}(x)-y)\\phi(x)  \\\\\n",
    "                                                 \\\\\n",
    "\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w}) &= \\nabla_\\mathbf{w}\\left(\\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y,g)\\in\\mathcal{D}_\\mathrm{train}} \\mathrm{Loss}(\\mathbf{w}, x, y)\\right) \\\\\n",
    "                                                &= \\frac{1}{|\\mathcal{D}_\\mathrm{train}|} \\sum_{(x,y,g)\\in\\mathcal{D}_\\mathrm{train}} \\nabla_\\mathbf{w}\\mathrm{Loss}(\\mathbf{w}, x, y)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ecd59-a9e6-49e2-bb72-caa7f165c5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_loss(w, x, y):\n",
    "    return 2 * (predict(w, x) - y) * phi(x)\n",
    "\n",
    "\n",
    "def gradient_train_loss(w):\n",
    "    total = sum(gradient_loss(w, x, y) for x, y, g in train_examples)\n",
    "    return total / len(train_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9139b0-289e-4777-a4bc-ca4ae01ab326",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\eta &\\gets 0.1 \\\\\n",
    "\\mathbf{w} &\\gets [0] \\\\\n",
    "\\mathbf{w} &\\gets \\mathbf{w} - \\eta\\nabla_\\mathbf{w}\\mathrm{TrainLoss}(\\mathbf{w})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904165c-2ea1-4780-9e37-7c8585ddc38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradientDescent(loss, gradient_loss, eta=0.1, w=np.array([0.0]), N=20):\n",
    "    for i in range(N):\n",
    "        w = w - eta * gradient_loss(w)\n",
    "        # print(f\"epoch={i} w={w} loss={loss(w)} loss={gradient_loss(w)}\")\n",
    "    return w, loss(w), gradient_loss(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f50dbe-2064-489d-aa07-4183cb1b6259",
   "metadata": {},
   "source": [
    "Obtenemos un buen modelo para los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65696754-543e-4a37-8c7c-40c048027414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w, lossW, gradientLossW = gradientDescent(train_loss, gradient_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b2af7-67be-4ae9-ade9-bfb5aa66eac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "xs1 = np.array([x for x, y, g in train_examples if g == \"A\"])\n",
    "ys1 = np.array([y for x, y, g in train_examples if g == \"A\"])\n",
    "xs2 = np.array([x for x, y, g in train_examples if g == \"B\"])\n",
    "ys2 = np.array([y for x, y, g in train_examples if g == \"B\"])\n",
    "\n",
    "pxs = np.linspace(0, 8, 100)\n",
    "pys = w[0] * pxs\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_xlim(0, 8.25)\n",
    "ax.set_ylim(0, 8.25)\n",
    "ax.set_xticks(range(9))\n",
    "ax.set_yticks(range(9))\n",
    "\n",
    "ax.scatter(xs1, ys1, label=\"A\")\n",
    "ax.scatter(xs2, ys2, label=\"B\")\n",
    "ax.plot(pxs, pys, color=\"black\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425d95e-b50d-4aff-8d9f-4d5ccb6d6cea",
   "metadata": {},
   "source": [
    "驴C贸mo se ve la minimizaci贸n de la p茅rdida promedio de entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38f563-511d-461f-ada4-da3759b3f174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "ws = np.linspace(0, 2, 100)\n",
    "ls = np.array([train_loss(np.array(w)) for w in ws])\n",
    "\n",
    "ax.set_xlabel(\"$w$\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(0, 80.25)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(0, 81, 20))\n",
    "\n",
    "ax.plot(ws, ls, c=\"r\", label=\"TrainLoss\")\n",
    "ax.scatter(w, lossW, c=\"r\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db3d33-c310-4018-b7e3-25e9218af1ce",
   "metadata": {},
   "source": [
    "Analicemos la p茅rdida promedio de entrenamiento **por grupo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e1a36-b0b2-485c-ad87-54cf7de8853e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loss_g(w, group):\n",
    "    examples = [loss(w, x, y)\n",
    "                for x, y, g \n",
    "                in train_examples\n",
    "                if g == group]\n",
    "    return sum(examples) / len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6637f4c-f3f7-4965-a5ed-33ca7c7a7799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "ws = np.linspace(0, 2, 100)\n",
    "lsA = np.array([train_loss_g(np.array(w), 'A') for w in ws])\n",
    "lsB = np.array([train_loss_g(np.array(w), 'B') for w in ws])\n",
    "\n",
    "ax.set_xlabel(\"$w$\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(0, 80.25)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(0, 81, 20))\n",
    "\n",
    "ax.plot(ws, lsA, label=\"$\\mathrm{TrainLoss}_A$\")\n",
    "ax.plot(ws, lsB, label=\"$\\mathrm{TrainLoss}_B$\")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ede51-3e4b-49c0-8c1a-c2fb117d0591",
   "metadata": {},
   "source": [
    "\n",
    "$$\\mathrm{TrainLoss}_\\mathrm{max}(\\mathbf{w}) = \\max_g\\mathrm{TrainLoss}_g(\\mathbf{w})$$\n",
    "\n",
    "$$\\nabla_\\mathbf{w}\\mathrm{TrainLoss}_\\mathrm{max}(\\mathbf{w}) = \\nabla_\\mathbf{w}\\mathrm{TrainLoss}_{g^\\ast}(\\mathbf{w})$$\n",
    "\n",
    "$$g^\\ast = \\mathrm{argmax}_g\\mathrm{TrainLoss}_g(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f2e9a-64b7-468b-a738-451d4d676ae4",
   "metadata": {},
   "source": [
    "# Predictores no-lineales\n",
    "\n",
    "Hemos discutido las componentes principales de problemas de regresi贸n y clasificaci贸n.\n",
    "\n",
    "1. Datos de entrenamiento $\\mathcal{D}$, nuestro conjunto de ejemplos con entradas $x$ y salidas $y$\n",
    "2. Clase de hip贸tesis $\\mathcal{F}$, el conjunto de todos los modelos de predicci贸n que consideramos en el aprendizaje\n",
    "3. Funci贸n de p茅rdida $\\mathrm{Loss}(\\mathbf{w}, x, y)$, mide qu茅 tan mal predice el modelo inducido por $\\mathbf{w}$ al ejemplo con entrada $x$ y salida $y$\n",
    "4. Algoritmo de optimizaci贸n, como descenso de gradiente, nos permite encontrar un modelo que minimiza la p茅rdida de los datos de entrenamiento\n",
    "\n",
    "En particular, los modelos que discutimos son lineales, esto en el sentido que el c谩lculo fundamental que realizan es $\\mathbf{w}\\cdot\\phi(x)$.\n",
    "\n",
    "La funci贸n $\\phi$ es llamada el extractor de caracter铆sticas y $\\phi(x)$ es llamado vector de caracter铆sticas.\n",
    "\n",
    "En el caso de la regresi贸n lineal, consideramos modelos parametrizados por un vector de pesos con dos componentes reales y $\\phi(x) = [1, x]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38e9b0-48c9-446b-bf99-b512f16fd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grafica(ws, colores):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    phi = lambda x: np.array([1.0, x])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = [np.array([np.dot(w, phi(x)) for x in xs]) for w in ws]\n",
    "    cs = mpl.colormaps[colores](np.linspace(0,1,len(ys)+2))[1:]\n",
    "    for y, c in zip(ys, cs):\n",
    "        ax.plot(xs, y, color=c)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ad3bf-e29d-467f-99dc-bbe76f7eaab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([2.0, 0.75 - 0.25 * i]) for i in range(5)], \"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8265-49fb-4b8c-819e-0400d4952ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([1 + i, 0.25]) for i in range(5)], \"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf25d1-8d04-438d-9378-2e25e9994f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([1.0 + 0.5 * i, 1.0 - 0.25 * i]) for i in range(5)], \"Oranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1e24b-0a2c-459d-8dc8-eaacb2e573df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grafica([np.array([1.0 + 0.5 * np.random.rand() * i, 1.0 - 0.25 * np.random.rand() * i]) for i in range(5)], \"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952de5fe-6394-4279-8cfa-fe7381abcd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48729d29-95ba-488c-8e66-86f1e19bfeee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "fig = plt.figure();\n",
    "ax = plt.axes();\n",
    "def plotline(w1=2.0, w2=1.0):\n",
    "    ax.clear()\n",
    "    w = np.array([w1, w2])\n",
    "    phi = lambda x: np.array([1.0, x])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = np.array([np.dot(w, phi(x)) for x in xs])\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);\n",
    "    fig.show();\n",
    "\n",
    "w = interactive(plotline, w1=(0.0, 6.0), w2=(-10.0, 10.0))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af6d64-e43c-4509-9394-28927ec6b973",
   "metadata": {},
   "source": [
    "Consideremos utilizar un extractor de caracter铆sticas cuadr谩tico...\n",
    "\n",
    "$$\\phi(x) = [1, x, x^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f6a95-8a92-4681-9764-b172fc41431f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure();\n",
    "ax = plt.axes();\n",
    "def plotquadratic(w1=2.0, w2=1.0, w3=-0.5):\n",
    "    ax.clear()\n",
    "    w = np.array([w1, w2, w3])\n",
    "    phi = lambda x: np.array([1.0, x, x**2])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = np.array([np.dot(w, phi(x)) for x in xs])\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);\n",
    "    fig.show();\n",
    "\n",
    "w = interactive(plotquadratic, w1=(0.0, 6.0), w2=(-10.0, 10.0), w3=(-2.0,2.0))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce489bf-a85a-4286-a963-da8b6fcb3867",
   "metadata": {},
   "source": [
    "**驴Qu茅 acaba de ocurrir?**\n",
    "\n",
    "El extractor de caracter铆sticas $\\phi$ puede ser arbitrario. Entonces inclu铆mos un t茅rmino adicional $x^2$.\n",
    "\n",
    "Consideremos el c谩lculo principal del predictor $\\mathbf{w}\\cdot\\phi(x)$, esta relaci贸n es lineal con respecto a $\\mathbf{w}$ y $\\phi(x)$. Pero no necesita ser lineal en $x$, las entradas pudieran ser incluso objetos distintos a los vectores.\n",
    "\n",
    "El algoritmo de aprendizaje observa a $\\phi(x)$ pero no a $x$.\n",
    "\n",
    "**驴Qu茅 otras maniacadas podemos conjurar?**\n",
    "\n",
    "$$\\phi(x) = [1, x, x^2, \\cos{(3x)}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82482fd-4dae-4870-a6cb-fec8ec6ba201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure();\n",
    "ax = plt.axes();\n",
    "def plotquadratic(w1=2.0, w2=1.0, w3=-0.5, w4=0.0):\n",
    "    ax.clear()\n",
    "    w = np.array([w1, w2, w3, w4])\n",
    "    phi = lambda x: np.array([1.0, x, x**2, np.cos(3*x)])\n",
    "    xs = np.linspace(0, 5, 100)\n",
    "    ys = np.array([np.dot(w, phi(x)) for x in xs])\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlim([0, 5]);\n",
    "    ax.set_ylim([0, 6]);\n",
    "    fig.show();\n",
    "\n",
    "w = interactive(\n",
    "    plotquadratic,\n",
    "    w1=(0.0, 6.0),\n",
    "    w2=(-10.0, 10.0),\n",
    "    w3=(-2.0, 2.0),\n",
    "    w4=(-1.0, 1.0),\n",
    ")\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a4c10-85f7-48a0-807a-dc937b3b7d87",
   "metadata": {},
   "source": [
    "Usualmente, la elecci贸n de caracter铆sticas depende de la tarea de predicci贸n que nos interesa resolver.\n",
    "\n",
    "Por ejemplo, si $x$ representa el tiempo y tenemos la sospecha que la salida real $y$ var铆a de acuerdo a alguna estructura peri贸dica, entonces podemos incorporar caracter铆sticas peri贸dicas como el coseno para capturar esas tendencias.\n",
    "\n",
    "Cada caracter铆stica puede representar alg煤n tipo de estructura en los datos. Las caracter铆sticas que incluyamos en nuestro predictor representan las propiedades que **pudieran** ser 煤tiles para la predicci贸n. Si una de estas no es 煤til, el algoritmo de aprendizaje puede *asignarle* un peso cercano a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfed41d-0629-4f2c-a8e9-993836f8f0cd",
   "metadata": {},
   "source": [
    "Retoma lo que discutimos sobre clasificadores lineales y considera las siguientes modificaciones:\n",
    "\n",
    "$$\\phi(x) = [x_1, x_2, x_1^2 + x_2^2]$$\n",
    "\n",
    "$$f_\\mathbf{w}(x) = \\mathrm{sign}(\\mathbf{w}\\cdot\\phi(x))$$\n",
    "\n",
    "Explora qu茅 clasificamos con el modelo parametrizado por $\\mathbf{w} = [2, 2, -1]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
